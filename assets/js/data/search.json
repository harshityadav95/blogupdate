[ { "title": "Notes Deployment Strategies", "url": "/posts/Deployment-Types/", "categories": "Software Development, Cloud", "tags": "deployment, webdevelopment", "date": "2023-05-17 02:30:00 +0800", "snippet": "SourceTLDR;Big Bang Deployment Big Bang deployment is a software release approach where the entire system is deployed in one go, replacing the existing system entirely. It involves high risk and ...", "content": "SourceTLDR;Big Bang Deployment Big Bang deployment is a software release approach where the entire system is deployed in one go, replacing the existing system entirely. It involves high risk and disruption since all users transition simultaneously. It requires thorough testing and preparation to ensure a smooth transition and rollback to previous version in case of deployment failure.Rolling Deployment Rolling deployment is a software release approach where updates are deployed gradually, with a subset of the system or users at a time. It involves deploying the new version while keeping the existing version running. It reduces the risk and impact of deployment by allowing for monitoring, testing, and addressing issues incrementally. It enables a smoother transition, as updates are applied in stages, ensuring minimal disruption to the overall system.Blue Green Deployment Blue Green Deployment is a release strategy where two identical environments, “blue” and “green,” are maintained. The blue environment represents the current production version, while the green environment is an exact replica of where the new version is deployed. Traffic is initially directed to the blue environment. Once the green environment is tested and validated, the switch is made, redirecting traffic from blue to green. This approach allows for zero-downtime deployments, easy rollback, and quick recovery in case of issues, as the previous version is still available in the blue environment.Canary Deployment Canary Deployment is a release strategy where a new version of the software is deployed to a small subset of users or servers, known as the “canary group.” This group receives the updated version while the rest of the users continue with the existing version. It allows for testing and monitoring the new version in a real-world environment, ensuring its stability and performance. If the canary group experiences no issues, the new version is gradually rolled out to the remaining Canary + Rolling Deployment ensures the best resultsFeature Deployment Feature Deployment is a release strategy where specific features or functionalities are deployed to a subset of users or environments. It involves enabling new features for a targeted audience while keeping the existing functionality intact for others. This approach allows for incremental rollout and testing of new features, gathering feedback, and addressing issues before a broader release. It offers flexibility, scalability, and the ability to iterate on features based on user feedback and requirements. Feature Toggle can be used in combination with any other deployment strategies.Reference : https://docs.aws.amazon.com/whitepapers/latest/practicing-continuous-integration-continuous-delivery/deployment-methods.html https://www.opsmx.com/blog/advanced-deployment-strategies-devops-methodology/" }, { "title": "iPhone in India 2023", "url": "/posts/iPhone-in-India-2023/", "categories": "Tech", "tags": "iOS, Apple, iPhone", "date": "2023-05-16 02:30:00 +0800", "snippet": "One small post for the biggest tech fanboy moment Android vs iOSFirst 10 days Usage iOS Shortcoming : iPhones do not support the 4G Volte carrier Video Calling service in India The NFC in iPhones...", "content": "One small post for the biggest tech fanboy moment Android vs iOSFirst 10 days Usage iOS Shortcoming : iPhones do not support the 4G Volte carrier Video Calling service in India The NFC in iPhones cannot be used for Visa or any other tap-to-pay terminals in India for now App Notification category to selectively disable is not a thing in iOS When picking calls and Bluetooth is paired the call is default picked on phone many times until set in settings While calling a number switching retrying and switching SIM card is better implemented on Android Weather forecast in the iOS weather app is way too inaccurate as compared to Google weather feeds in Bangalore, India When call connection fails hard to troubleshoot the cause vowifi/volte etc ?DaytoDay irks Whatsapp cannot send photos as documents straight forwards as one can do it in Android Notification mangement in iOS is boon &amp; bane in disguise as notifications keep piling up easily without affecting screen on time" }, { "title": "Event Women Techmaker 2023", "url": "/posts/Event-Women-Techmaker-2023/", "categories": "Event", "tags": "event, google", "date": "2023-03-08 02:30:00 +0800", "snippet": "Event: Women TechMaker 2023Program Links Google Accelerator for Startups Women Developer AcademyConference LiveStreamMe @ event", "content": "Event: Women TechMaker 2023Program Links Google Accelerator for Startups Women Developer AcademyConference LiveStreamMe @ event" }, { "title": "Completing Ai 900", "url": "/posts/Microsoft-AI-900/", "categories": "Cloud", "tags": "Azure, webdevelopment", "date": "2023-01-29 02:30:00 +0800", "snippet": "Completion of CertificationCompleting this certification was mostly inspired by chatgpt and exploring all the available options and Ai ML services in azure and also all exploring deployment option...", "content": "Completion of CertificationCompleting this certification was mostly inspired by chatgpt and exploring all the available options and Ai ML services in azure and also all exploring deployment options for ML projects and backends in azurePreparation The text content of the exam is quite simple and the interesting part is to try hands-on the various ML tools and services provided by azure and try them out else from the exam point of view theory in the preparation module provided by azure is more then sufficient along with some sample question dumps If have worked in ML before and have an idea of the basics of azure and both ML the prep and exam can be prepared for in a single day-long stretch if hands-on or single hands on any of the services is therePractise Question SourceThe included question is sufficient to pass with an above 900+ score Source 1 Source 2 Source 3Taking the Exam The exam had 35 questions to be done in 45 minutes which is more the sufficient time to complete this exam with plenty of time to spareCertificate Credly link for Certificate" }, { "title": "DNS Domain Name Server Notes", "url": "/posts/DNS-Domain-Name-Server/", "categories": "Networking & Communication", "tags": "Web Development, Network", "date": "2023-01-03 02:30:00 +0800", "snippet": "DNS Domain Name Server: NoteDNS Resolver ProvidersWhat is Software that makes DNS Server work Bind V9 NDS KNOTS DNSWhy only 13 root DNS servers ? A common misconception is that there are only 1...", "content": "DNS Domain Name Server: NoteDNS Resolver ProvidersWhat is Software that makes DNS Server work Bind V9 NDS KNOTS DNSWhy only 13 root DNS servers ? A common misconception is that there are only 13 root servers in the world. In reality, there are many more, but still only 13 IP addresses used to query the different root server networks. Each has associated with it an IP address (and shortly some will have more than one as IPv6 is further rolled out) There are 13 logical root name servers specified, with logical names in the form letter.root-servers.net, where the letter ranges from a to m. which specifies a maximum packet size of 512 bytes when using the User Datagram Protocol (UDP). Technically, however, fourteen name servers fit into an IPv4 packet. The addition of IPv6 addresses for the root name servers requires more than 512 bytes, which is facilitated by the EDNS0 extension to the DNS standard.Types of DNS Servers Recursive server Root name server TLD serve Authoritative name serverReference : types of dns serversDNS Query FlowThe diagram below illustrates how DNS lookup works under the hood: harshityadav.in is typed into the browser, and the browser sends the domain name to the DNS resolver. The resolver queries a DNS root name server. The root server responds to the resolver with the address of a TLD DNS server. In this case, it is .in. The resolver then makes a request to the .in TLD. The TLD server responds with the IP address of the domain’s name server, harshityadav.in (authoritative name server). The DNS resolver sends a query to the domain’s nameserver. The IP address for harshityadav.in is then returned to the resolver from the nameserver. The DNS resolver responds to the web browser with the IP address (142.251.46.238) of the domain requested initially.DNS lookups on average take between 20-120 milliseconds to complete (according to YSlow).DNS CachingDNS by Cloud Providers Azure : Azure DNS » Azure Private DNS » Azure DNS Private Resolver Google: Cloud DNS Amazon: Amazon Route 53DNS over HTTPS (DOH) : DNS maps a domain name to an IP address, which is done by sending a UDP packet to the DNS resolver on port 53 The data is sent un-encrypted and ISP can snoop in DNS over HTTPS establishes a TLS connection with a DNS resolver to stop anyone from tracking DNS resolved have to decrypt to read the data hence ISP can monitorSolution: Oblivious HTTPS over DNS ODOH It adds a proxy layer in the middle so that the client interacts with proxy layer and the entire process is encrypted end to endRecapVideo ReferenceReference : https://www.cloudflare.com/learning/dns/glossary/dns-root-server/#:~:text=A common misconception is that,addresses in the root zone. https://www.lifewire.com/dns-root-name-servers-3971336 https://www.icann.org/en/blogs/details/there-are-not-13-root-servers-15-11-2007-en https://en.wikipedia.org/wiki/Root_name_server https://www.iana.org/domains/root/servers https://www.lifewire.com/what-is-a-dns-server-2625854 https://blog.bytebytego.com/p/how-does-the-domain-name-system-dns" }, { "title": "JVM Class Loader Notes", "url": "/posts/Java-JVM-ClassLoader/", "categories": "java", "tags": "java, jvm", "date": "2023-01-02 02:30:00 +0800", "snippet": "Class Loader in Java: NotesThe Java ClassLoader is a part of the Java Runtime Environment that dynamically loads Java classes into the Java Virtual Machine.JVM doesn’t need to know about the underl...", "content": "Class Loader in Java: NotesThe Java ClassLoader is a part of the Java Runtime Environment that dynamically loads Java classes into the Java Virtual Machine.JVM doesn’t need to know about the underlying files or file systems in order to run Java programs thanks to class loaders. Java ClassLoader is an abstract class It belongs to a java.lang package It loads classes from different resources. Java ClassLoader is used to load the classes at run time.Java classes aren’t loaded into memory all at once, but when required by an application. At this point, the Java ClassLoader is called by the JRE, and these ClassLoaders load classes into memory dynamically. When we request to load a class, it delegates the class to its parent. In this way, uniqueness is maintained in the runtime environment. It is essential to execute a Java program.Java ClassLoader is based on three principles: Delegation, Visibility, and Uniqueness. Delegation principle: It forwards the request for class loading to parent class loader. It only loads the class if the parent does not find or load the class. Visibility principle: It allows child class loader to see all the classes loaded by parent ClassLoader. But the parent class loader cannot see classes loaded by the child class loader. Uniqueness principle: It allows to load a class once. It is achieved by delegation principle. It ensures that child ClassLoader doesn’t reload the class, which is already loaded by the parent.When classes are loadedThere are only two cases: When the new byte code is executed. When the byte code makes a static reference to a class. For example, System.out.Static vs. Dynamic Class Loading Classes are statically loaded with “new” operator. Dynamic class loading invokes the functions of a class loader at run time by using Class.forName() method.Types of ClassLoaderIn Java, every ClassLoader has a predefined location from where they load class files. There are following types of ClassLoader in Java:class Demo { public static void main(String[] args) { System.out.println(\"Demo World\"); System.out.println(\"Classloader of ArrayList:\" + ArrayList.class.getClassLoader()); System.out.println(\"Classloader of Demo:\" + Demo.class.getClassLoader()); }}As we can see, there are three different class loaders here: application, extension, and bootstrap (displayed as null).Class loader of this class:sun.misc.Launcher$AppClassLoader@18b4aac2Class loader of Logging:sun.misc.Launcher$ExtClassLoader@3caeaf62Class loader of ArrayList:null The application class loader loads the class where the example method is contained. An application or system class loader loads our own files in the classpath. Next, the extension class loader loads the Logging class. Extension class loaders load classes that are an extension of the standard core Java classes. Finally, the bootstrap class loader loads the ArrayList class. A bootstrap or primordial class loader is the parent of all the others.However, we can see that for the ArrayList, it displays null in the output. This is because the bootstrap class loader is written in native code, not Java, so it doesn’t show up as a Java class.As a result, the behavior of the bootstrap class loader will differ across JVMs.1. Bootstrap Class Loader: The bootstrap class loader is rigidly defined in the JVM and loads class files according to the specification. First, the Java Virtual Machine determines whether the bootstrap class loader has already been recorded as an initiating loader of a class or interface Java Virtual Machine passes the argument N to an invocation of a method on the bootstrap class loader to search for a purported representation of Class in a platform-dependent manner. a class or interface will be represented using a file in a hierarchical file system, and the name of the class or interface will be encoded in the pathname of the file. If no purported representation of C is found, loading throws an instance of ClassNotFoundException. It loads standard JDK class files from rt.jar and other core classes. It is a parent of all class loaders. It doesn’t have any parent. When we call String.class.getClassLoader() it returns null, and any code based on it throws NullPointerException. question is, who loads the java.lang.ClassLoader itself? ** This is where the bootstrap or primordial class loader comes into play. It is also called Primordial ClassLoader. It loads class files from jre/lib/rt.jar. For example, java.lang package class. This bootstrap class loader is part of the core JVM and is written in native code,2. Extensions Class Loader: The extension class loader is a child of the bootstrap class loader, and takes care of loading the extensions of the standard core Java classes so that they’re available to all applications running on the platform. It delegates class loading request to its parent. If the loading of a class is unsuccessful, it loads classes from jre/lib/ext directory or any other directory as java.ext.dirs. The extension class loader loads from the JDK extensions directory, usually the $JAVA_HOME/lib/ext directory, or any other directory mentioned in the java.ext.dirs system property. It is implemented by sun.misc.Launcher$ExtClassLoader in JVM.3. System Class Loader: It loads application specific classes from the CLASSPATH environment variable. It can be set while invoking program using -cp or classpath command line options. It’s also a child of the extensions class loader. It is a child of Extension ClassLoader. It is implemented by sun.misc.Launcher$AppClassLoader class. All Java ClassLoader implements java.lang.ClassLoader.How ClassLoader works in Java Class loaders are part of the Java Runtime Environment. When the JVM requests a class, the class loader tries to locate the class and load the class definition into the runtime using the fully qualified class name. When JVM request for a class, it invokes a loadClass() method of the java.lang.ClassLoader class by passing the fully classified name of the class. The loadClass() method calls for findLoadedClass() method to check that the class has been already loaded or not. It is required to avoid loading the class multiple times. If the class is already loaded, it delegates the request to parent ClassLoader to load the class. If the ClassLoader is not finding the class, it invokes the findClass() method to look for the classes in the file system.The following diagram shows how ClassLoader loads class in Java using delegation.Example of Class Loader WorkingSuppose that we have an application-specific class Demo.class. The request for loading of this class files transfers to Application ClassLoader. It delegates to its parent Extension ClassLoader. Further, it delegates to Bootstrap ClassLoader. Bootstrap search that class in rt.jar and since that class is not there. Now request transfer to Extension ClassLoader which searches for the directory jre/lib/ext and tries to locate this class there. If the class is found there, Extension ClassLoader loads that class. Application ClassLoader never loads that class. When the extension ClassLoader does not load it, then Application ClaasLoader loads it from CLASSPATH in Java.Visibility principle states that child ClassLoader can see the class loaded by the parent ClassLoader, but vice versa is not true. It means if Application ClassLoader loads Demo.class, in such case, trying to load Demo.class explicitly using Extension ClassLoader throws java.lang.ClassNotFoundException.According to the uniqueness principle, a class loaded by the parent should not be loaded by Child ClassLoader again. So, it is possible to write class loader which violates delegation and uniqueness principles and loads class by itself.In short, class loader follows the following rule: It checks if the class is already loaded. If the class is not loaded, ask parent class loader to load the class. If the parent class loader cannot load class, attempt to load it in this class loader.Code Example :create a file classloader.javaclass Demo { public static void main(String[] args) { System.out.println(\"Demo World\"); }}Compile the code using$ javac classloader.java$ java -verbose:class Demo# -verbose:class: It is used to display the information about classes being loaded by JVM. It is useful when using class loader for loading classes dynamically. The following figure shows the output. We can observe that runtime classes required by the application class (Demo) are loaded first.Methods of Java.lang.ClassLoader https://www.geeksforgeeks.org/classloader-in-java/Custom Class Loader Use-CasesCustom class loaders are helpful for more than just loading the class during runtime. A few use cases might include: Helping to modify the existing bytecode, e.g. weaving agents Creating classes dynamically suited to the user’s needs, e.g. in JDBC, switching between different driver implementations is done through dynamic class loading. Implementing a class versioning mechanism while loading different bytecodes for classes with the same names and packages. This can be done either through a URL class loader (load jars via URLs) or custom class loaders. Creating Custom Class Loader Example It is open for vendor-specific implementation and can custom load classes via the java.lang.Class instance. Every user-defined class loader is an instance of a subclass of the abstract class ClassLoader. Applications employ user-defined class loaders in order to extend the manner in which the Java Virtual Machine dynamically loads and thereby creates classes. User-defined class loaders can be used to create classes that originate from user-defined sources. For example, a class could be downloaded across a network, generated on the fly, or extracted from an encrypted file. If the class loader L is unable to load a class or interface denoted by N for any reason, it must throw an instance of ClassNotFoundException. At run time, a class or interface is determined not by its name alone, but by a pair: its binary name + and its defining class loader. For example, the normal binary name of class Thread is java.lang.Thread. In the internal form used in descriptors in the class file format, a reference to the name of class Thread is implemented using a CONSTANT_Utf8_info structure representing the string java/lang/Thread. Each such class or interface belongs to a single run-time package The run-time package of a class or interface is determined by the package name and defining class loader of the class or interface. If an error occurs during class loading, then an instance of a subclass of LinkageError must be thrown at a point in the program that (directly or indirectly) uses the class or interface being loaded.**Context Classloaders**However, sometimes when JVM core classes need to dynamically load classes or resources provided by application developers, we might encounter a problem.For example, in JNDI,(Java Naming and Directory Interface) the core functionality is implemented by the bootstrap classes in rt.jar.  But these JNDI classes may load JNDI providers implemented by independent vendors (deployed in the application classpath). This scenario calls for the bootstrap class loader (parent class loader) to load a class visible to the application loader (child class loader). The java.lang.Thread class has a method, getContextClassLoader(), that returns the ContextClassLoader for the particular thread. The ContextClassLoader is provided by the creator of the thread when loading resources and classes.J2SE delegation doesn’t work here, and to get around this problem, we need to find alternative ways of class loading. This can be achieved using thread context loaders.Difference between loadClass() and Class.forName() The loadClass() method loads only the class but does not initialize the object. While Class.forName() method initialize the object after loading it. For example, if you are using ClassLoader.loadClass() to load the JDBC driver, class loader does not allow to load JDBC driver. The java.lang.Class.forName() method returns the Class Object coupled with the class or interfaces with the given string name. It throws ClassNotFoundException if the class is not found.Reference https://www.javatpoint.com/classloader-in-java https://www.baeldung.com/java-classloaders https://www.geeksforgeeks.org/classloader-in-java/ https://www.baeldung.com/java-classloaders#context-classloaders" }, { "title": "Java Revision Topic Sets", "url": "/posts/Java-Revision-Topic-Sets/", "categories": "Java", "tags": "Programming, Interview Preparation", "date": "2023-01-01 02:30:00 +0800", "snippet": "Java RoadMap [InUpdate]SET 1: JVM Architecture Classes + Objects Inheritance/ Enum/ Polymorphism / Overloading / Overriding Transient / Assert / Volatile Interface ...", "content": "Java RoadMap [InUpdate]SET 1: JVM Architecture Classes + Objects Inheritance/ Enum/ Polymorphism / Overloading / Overriding Transient / Assert / Volatile Interface / Abstrasct class / Inner class / Anonymous class / Enum Wrapper classes / Boxing / un-boxing Marker interfaces / tagged interfaces Static / final / implements / Extends Object class inherited methods. label Super and this key word SET 2: Java.lang pkg / Java.util pkg String / StringBuffer / BufferReader / BufferWritter instance of / clone / toString() / HashCode() / Hasing Exceptions/packages / throws/throw / Hierarchy / Customer Exceptions / Exceptions with method overriding / finally block related questions. checked and un-checked exceptions / Extending throwable interface and exception class / SET 3: Life Cycle of thread Threading -&gt; Priority thread Deamon thread Runnable callable Executer service Future task Yield() / Stop() / concurrency / weight notify Inter thread communication Thread group / Thread pool / thread safety / thread moditer / shut down hook. Joining of thread / fork() performing multiple task. Runtime class. Locks Synchronization: Synchronization in java synchronized block / Deadlock in java / interrupting thread / Concurrent MAP / priority blocking queue. SET 4: Java input/Output. Input / Output stream Console File reader / writer filterinput / Output stream Buffered reader / writer print stream print writer. push back input stream string writer/reader. Serializable / Externalizable Object reader/ writer - Hierarchy Regular expressions. Singletons SET 5: Collections: Java - Collections - Set, List, Map, Hashset &amp; HashMap Hierarchy Collection of MAP interface Complexiy for all Collections best/worst cases. iterable / comparable / comparator Custom implementation list/ linkedlist/ hashmap / hashset jagged array. SET 6: Socket programming / RMI / EJB JDBC and related classes and SQL interview questions JSP, Servlets, Filters, Servlet Config, Servlet Context - instance related, implicit variables, GET and POST Transactions Webservices Some Java Motivation Video : World Without Java Java Forever And Ever Movie (Java vs Windows .Net) " }, { "title": "Web Socket Overview", "url": "/posts/Web-Socket-Overview/", "categories": "Networking & Communication", "tags": "Web Development, Network", "date": "2022-06-08 02:30:00 +0800", "snippet": "What is Web Socket It is a Bidirectional, full Duplex protocol to communicate between client and server over the web It is used in Chatting, notifications, real-time feeds, multiplayer gamingHTT...", "content": "What is Web Socket It is a Bidirectional, full Duplex protocol to communicate between client and server over the web It is used in Chatting, notifications, real-time feeds, multiplayer gamingHTTP 1.0 HTTP Protocol 1.0 Recap, is built on TCP protocol when it was first built as a request-response system The Server cannot randomly send requests to clients it needs to be requested for something to respond to ie client has to initiate the request first After Each Request, the connection is closed and for each request, the connection has to be established again Consider the example where the client requests 10 images for each request the server will (open and close connection) x10 for each image to receive which is costly for the server and the clientHTTP 1.1 Realizing the previous implementation of HTTP 1.1 was made which once made a request to keep the connection open It keeps the connection open using the Header “ keep-alive” in the header which is an epithermal header and cannot be propagated through proxies Once a Connection is made it remains open and subsequent requests can be made using the same TCP connection We Close the connection once not neededWeb Sockets Web Socket uses HTTP 1.1 between client and server It’s a stateful protocol since the client and server are both aware of each other HTTP is Stateless We open a connection and do a Websocket Handshake Once the WebSocket handshake happens it detach from HTTP and becomes a binary protocol Anyone can send data to anyone, using the underlying TCP connection API The client can send all data to the server The server can send all data to the client Both can concurrently send data to each other etc etcWebSockets Handshake WS:// or wss:// similar to HTTP and HTTPS The First request we make is normal HTTP GET request, which asks the server to upgrade to 1.1 The Server will reply 101 Switching protocol, and become binary protocol The key is used for seeding and hashing to generate a new key Can you use the Existing Server or a do it manually to implement the sameWebSockets use cases Chat Application Live Feed Multiplayer gaming (Multiple clients means maintaining multiple states ie scaling stateful is difficult ) Showing client progress/loggingExample Implementing WebSockets in Node JSWeb Sockets Pros and ConsPros Full-duplex (no polling), not have to ping the server all the time to get some status or progress update HTTP compatible Firewall friendly (standard) (80 if Insecure ) (443 when Secure)Cons Proxying is tricky L7, have to break the connection and create another to the backend (L4 works better) L7 load balancing challenging (timeouts) Stateful, difficult to horizontally scaleDo we Have to use a Web Proxy? No !, Rule of thumb - do we absolutely need bi-directional communication Long polling Event SourceReference Video WebSockets Crash Course - Handshake, Use-cases, Pros &amp; Cons and more What Happens using WebSocket Connection" }, { "title": "TLS Termination Proxy", "url": "/posts/TLS-Termination-Proxy/", "categories": "Networking & Communication", "tags": "Web Development, Network", "date": "2022-06-07 02:30:00 +0800", "snippet": "TLS or SSL Termination proxy is a proxy that terminates the TLS session and sends the unencrypted traffic to the main server This offloads the load of the task of decrypting the packet from the m...", "content": "TLS or SSL Termination proxy is a proxy that terminates the TLS session and sends the unencrypted traffic to the main server This offloads the load of the task of decrypting the packet from the main server to the proxy serverTLS 1.2 Recap The goal is to establish an HTTPS connection Before that, we establish a TCP connection Client Sends the Hello message telling which all security versions, and protocols it supports The server agrees to it and sends the public key back to the client The client will create the key and encrypt it with the server public key and send it across The Key has to be symmetric since it is faster and less costlyTLS Termination Proxy A proxy that is facing the client and the proxy will agree on the key and exchange encrypted traffic The Proxy could be a load balancer/service mesh proxy We can skip the proxy but we add it for better analytics, intrusion system, caching, etc We decrypt the data for better decision, load balancing (L7) Once we terminate the proxy we send the data to the backend server unencryptedTLS Forward Proxy To encrypt the traffic between proxy to backend we can encrypt it again We can negotiate another proxy session between proxy and backend This adds cost to the backend server for encryption The majority of clouds (Azure, AWS )use TLS forward proxy but we can keep it decrypted if the backend server is internal, etcTLS Termination Pros and ConsPros Offloads crypt processing, faster for backend comparatively TLS close to the client (in the case of TLS 1.2 does not matter in 1.3) (can be made geographically ) Can Use HTTP Accelerator (Varnish), rewrite JS, compress, optimize Intrusion Detection system Load Balancing / Service MeshCons Limited by the max connection of Proxy (between client and balancer and balancer and backend), L4 does not have this problem, have to use multiple proxies or extra L4, or DNS load balancer, etc for management If Compromised all data is availableResource : Video https://youtu.be/H0bkLsUe3no" }, { "title": "Load Balancing Layer 4 vs Layer 7", "url": "/posts/Load-Balancing-Layer-4-vs-Layer-7/", "categories": "Networking & Communication", "tags": "Web Development, Network", "date": "2022-06-06 02:30:00 +0800", "snippet": "What is Load BalancingLoad balancing is the process of balancing incoming requests to multiple machines, process or services in the backendLayer 4 vs Layer 7to get started start thinking let’s work...", "content": "What is Load BalancingLoad balancing is the process of balancing incoming requests to multiple machines, process or services in the backendLayer 4 vs Layer 7to get started start thinking let’s work on this understanding Layer 3+ Layer 4 almost work together and intertwined Layer 3 and 4 is where TCP/UDP or IPV4 or IPv6 takes place Layer 7 is where the HTTP , SMTP , FTP , Headers , Cookies are Load Balancer can be a dedicated machine or virtual machine or software on a desktopLayer 4 Load BalancerIn Layer 4 load balancer we know only the IP address or the port number so that is the only data visible to us, we don’t know the data only the certain segments are visible to us the data might be encrypted or scrambled so cannot read the data at this layer It makes decision made on the algorithm such as round-robin or list connection etc to select the target machine it want to select for forwarding It keeps a Network Address Translation it keeps a table to keep the record of which server it sent the data too The Client only interacts with the server in front and not the machine the request is balanced to. (reverse proxy) So the Machine at the end does not know the incoming address of the client and the client does not know the actual machine that is processing/handling the requestLayer 4 Proxy Pros and ConsPros Simple Load balancing Efficient (no data lookup) More Secure (Does not decrypt the data and read the data) One TCP Connection (between the destination and the source) Use NAT (stateful)Cons No Smart Load balancing (cannot re-write URL, cookies, etc) Not Applicable for Microservices as these forward service requests based on the path as /auth will take to other server /setting will go to other server or backend *Sticky per segment (if a large request is coming all the requests has to be sent to the same server destination and cannot send part of the packet segment to one server) Once a connection is established, it goes to one server at the backend. All packets flowing to this connection go to one server. The next connection will then pick another server based on the algorithm. No Caching since cannot cache it Maintains only one connection between client and server NATed so your load balancer can serve a maximum number of TCP connections = to (number of servers * max connections per server)Layer 7 Load Balancer Can decrypt the data and use it to make the decision to forward the data Same Reverse Proxy concept to hide the backend Client ←→ Connection to balanced (Ingres) ←→ connection to the backend Two connections are used the client handshakes with the balanced and the balancer establishes a connection with the backend to communicatePros Smart Load Balancing Caching if /about is asked more that can be cached Great for MicroserviceCons More expensive Requires decrypting In terms of security, you have to share your certificate with the load balancers. If an attacker gets access to the load balancer, they automatically have access to all your data. Its proxy creates multiple connections—client, to proxy/proxy server—so you are bounded by the max TCP connection on your load balancer.Reference : Video https://youtu.be/aKMLgFVxZYk https://www.resonatenetworks.com/2020/04/29/layer-4-vs-layer-7-load-balancing/ https://medium.com/@harishramkumar/difference-between-layer-4-vs-layer-7-load-balancing-57464e29ed9f https://freeloadbalancer.com/load-balancing-layer-4-and-layer-7/ https://www.snapt.net/glossary/layer-4-vs-layer-7-load-balancing-explained" }, { "title": "Understanding Layer 4 vs Layer 7 Reverse Proxy", "url": "/posts/Understanding-Layer-4-vs-Layer-7-Reverse-Proxy/", "categories": "Networking & Communication", "tags": "Web Development, Network", "date": "2022-06-05 02:30:00 +0800", "snippet": "Proxy vs Reverse ProxyLayer 7 ProxyingConsider the scenario There is a client that wants to connect to Server H1 The client establishes a TCP connection to the server and completes the Handshake ...", "content": "Proxy vs Reverse ProxyLayer 7 ProxyingConsider the scenario There is a client that wants to connect to Server H1 The client establishes a TCP connection to the server and completes the Handshake The client sends the git request /employee to list all the names of the employee working in a company The total request we send is divided into 3 packets which are sent to the server The Server the 3 Packets and acknowledges receiving the 3 packets Only After the Server has received all the 3 packets it will consider the complete request as received since 1 request (in our case 3 packets all 3 have to reach the server to receiver complete request) Since the balancing is L7 the balancing server will be read by the server to read and decide which server to forward the request to Now after reading the data it will know the which server to hit in the backend Consider the H1 server implementing Round Robin Algorithm (next time it can send it to H2 it can do this since HTTP is stateless) H1 will establish a TCP connection to S1 (some reverse proxy already have TCP request and does not create it upon receiving the packet and already have a connection pool in place) Now the server makes brand new 3 request packets and sends them to backend S1, then these 3 requests once received by S1 it will start working on it Meanwhile, all this process is happening the client is waiting due to Asynchronous not blocked After the Backend S1 has finished processing the backend will send back the empty 2 packets (result response ) to the balancing server H1 which will be again made brand new packets and sent to the clientLayer 4 ProxyingConsider the following scenario The server sends the 3 packets to the balancing server H1 The Server is implemented a round-robin algorithm and picks the S1 server, establishes a connection with the S1 server, and immediately keeps sending all the incoming packets coming from the client to the S1 backend server as soon as they arrive L4 Proxy does not open the packet** to read the data inside it whether it’s/GET request or a /Post request coming to the server, just blindly forwarding the packets based on the IP Since its Sticky as long as the client address is the same all the requests coming from the same client will be sent to the same server After processing the request the S1 returns the response in 2 packets which are being sent back to the load balancer H1, which same packets are being sent to the client L4 proxy can work irrespective of the underlying protocolTLS in Layer 4 Proxy** (1 of implementation)Consider the following scenario The client sends the request to the H1 Server to establish a TCP connection Despite being Layer 4 Proxy the server needs to look at the packet to see the TCP connection type and establish a connection with the Backend H1 As the Hello Packets are being received by the H1 they are being sent to S1 and the Hello packets response sent by S1 back to H1 are immediately sent back to the client So during the Handshake, the Client will know the key and the backend S1 will know the key but the H1 balancer in between will not know the key The Certificate is Sent by S1 and not the H1 balancer There are other implementations where the S1 does not have the TLS and H1 will have to terminate the connectionReference : Video https://youtu.be/ylkAc9wmKhc" }, { "title": "Understanding JVM Arguments", "url": "/posts/Understanding-JVM-Arguments/", "categories": "Java", "tags": "Software Development, Java", "date": "2022-06-04 02:30:00 +0800", "snippet": "Quick Recall of JVM , JRE and JDKWhat are JVM ArgumentsA string that contains the arguments that are passed to the JVM that the driver is startingThe generic Java™ virtual machine (JVM) arguments a...", "content": "Quick Recall of JVM , JRE and JDKWhat are JVM ArgumentsA string that contains the arguments that are passed to the JVM that the driver is startingThe generic Java™ virtual machine (JVM) arguments are optional command-line arguments that are passed to the JVM when Application Server starts. The generic JVM arguments can set the - timeout value for the server-side Java remote method invocation (RMI) disable explicit garbage collection set the garbage collection policy, and specify the nursery size.User Defined ArgumentsStandard Options (-D but not only)These are the most commonly used options that are supported by all implementations of the JVM.You use -D to specify System properties but most of them don’t have any prefix :-verbose, -showversion, and so for.If you do not specify anything like -myProp=”XYZ” it means it is passed as an argument to main method of the program. D means you can use this value using System.getPropertyJVM MemoryWhen specifying the heap size for the JVM, note that the JVM tries to allocate the heap memory as a single contiguous range of addresses in the application’s memory address space.If the application’s address space is fragmented so that there is no contiguous range of addresses big enough for the amount of memory specified for the JVM, the driver fails to load, because the JVM cannot allocate its heap. This situation is typically encountered only with 32-bit applications, which have a much smaller application address space. If you encounter problems with loading the driver in an application, try reducing the amount of memory requested for the JVM heap. If possible, switch to a 64-bit version of the application.Topic Like these be OverwhelmingWhile working across application the popular ones one might come across for brief understanding we will glance through these 7:1. -Xmx and -XX:MaxMetaspaceSize Xmx is probably the most important JVM argument. Xmx defines the maximum amount of heap size you are allocating to your application. (To learn about different memory regions in a JVM, you may watch this short video clip). You can define your application’s heap size like this:2. GC AlgorithmAs of March 2020, there are 7 different GC algorithms in OpenJDK: Serial GC. Parallel GC. Concurrent Mark and Sweep GC. G1 GC. Shenandoah GC. Z GC. Epsilon GC.If you don’t specify the GC algorithm explicitly, then JVM will choose the default algorithm. Until Java 8, Parallel GC is the default GC algorithm. Since Java 9, G1 GC is the default GC algorithm.3. Enable GC LoggingGarbage Collection logs contain information about Garbage Collection events, memory reclaimed, pause time duration, etc. You can enable the garbage collection log by passing following JVM arguments:Typically, GC logs are used for tuning garbage collection performance. However, GC logs contain vital micro metrics. These metrics can be used for forecasting an application’s availability and performance characteristics.In this article, we would like to highlight one such micrometric: ‘GC Throughput‘ (to read more on other available micrometrics, you may refer to this article).4. -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPathOutOfMemoryError is a serious problem that will affect your application’s availability/performance SLAs. To diagnose OutOfMemoryError or any memory-related problems, one would have to capture the heap dump right at the moment or few moments before the application starts to experience OutOfMemoryError.As we don’t know when OutOfMemoryError will be thrown, it’s hard to capture the heap dump manually at the right around the time. However, capturing heap dumps can be automated by passing following JVM arguments:5. -XssEach application will have tens, hundreds, thousands of threads. Each thread will have its own stack. In each thread’s stack following information are stored: Methods/functions that are currently executed. Primitive datatypes. Variables. Object pointers. Return values.Each one of them consumes memory. If their consumption goes beyond a certain limit, then a StackOverflowError is thrown. More details about StackOverflowError and solutions to resolve it can be found in this article. However, you can increase the thread’s stack size limit by passing the -Xss6. -Dsun.net.client.defaultConnectTimeout and -Dsun.net.client.defaultReadTimeoutModern applications use numerous protocols (i.e. SOAP, REST, HTTP, HTTPS, JDBC, RMI, etc.) to connect with remote applications. Sometimes remote applications might take a long time to respond. Sometimes they may not respond at all.If you don’t have proper timeout settings, and if remote applications don’t respond fast enough, then your application threads/resources will get stuck. Remote applications unresponsiveness can affect your application’s availability. It can bring down your application to a grinding halt. To safeguard your application’s high availability, appropriate timeout settings should be configured.7. -Duser.timeZoneYour application might have sensitive business requirements around time/date. For example, if you are building a trading application, you can’t take a transaction before 9:30 am. To implement those time/date related business requirements, you might be using java.util.Date or java.util.Calendar objects.These objects, by default, pick up time zone information from the underlying operating system. This will become a problem if your application is running in a distributed environment.Example : If your application is running across multiple data centers, say, San Francisco, Chicago, Singapore – then JVMs in each data center would end up having a different time zone. Thus, JVMs in each data center would exhibit different behaviors. It would result in inconsistent results.Source : DZONEThere are 600+ arguments that you can pass to JVM just around garbage collection and memory. If you include other aspects, the number of JVM arguments will easily cross 1000+Example Xmn1024mSets the size of the nursery to 25% of the maximum heap size. The nursery is the area in the heap where objects are created. If you analyze the garbage collection and then adjust the heap sizes, adjust the nursery size to reflect your changes. XdisableexplicitgcDisables explicit garbage collection, which prevents System.gc() calls from starting the garbage collection process. Xgcpolicy:genconSets the garbage collection policy to gencon garbage collection, which places objects in separate areas of the heap based on their lifetime. After objects are created in the nursery and then survive a number of garbage collections, the objects are moved to a tenured area. When objects are separated in this way, garbage collection can run more frequently in the nursery without affecting the rest of the heap, which keeps pauses to a minimum. Because Maximo® Asset Management creates many short-lived objects, set the garbage collection policy to gencon.Reference https://www.baeldung.com/jvm-parameters https://dzone.com/articles/7-jvm-arguments-of-highly-effective-applications-1 https://www.youtube.com/watch?v=uJLOlCuOR4k&amp;t=9s https://www.youtube.com/watch?v=6G0E4O5yxks" }, { "title": "What is Result Set in Database ?", "url": "/posts/What-is-Result-Set-in-Database/", "categories": "Database Engineering", "tags": "Backend Development, Database", "date": "2022-06-03 02:30:00 +0800", "snippet": "The SQL statements that read data from a database query, return the data in a result set.OverviewConsider a situation that there are 12 million records of students in a database , if we implement ...", "content": "The SQL statements that read data from a database query, return the data in a result set.OverviewConsider a situation that there are 12 million records of students in a database , if we implement a query to retrieve the students with marks between 90 and 100 ;select count(*) from grades where grades between 90 and 100;the result is small with the just the count the value is still probably in millions at max , but if we run the query to return the ID of students with the marks matching the criteriaselect ID from grades where grades between 90 and 100;when we execute the following query the database is doing a lot for in the background (to create the execution plan first find out which index to use , and then do the actual fetching ) first finding those records which match the condition and then return record to the output machine in case of client server machine using the TCP Protocol (Depends on database) to send back Wait for all the result to come back and store in memory all the resultSo Imagine running a query that return millions of records ? how to handle and receive and store the results being sent by the databaseSolution to this : Database Cursors (Server Side Cursor , Yes there are server side cursors also)What are Database Cursor ? We create database on the cursor telling them that we are about to fetch the result of the query but do not give the result right now , just create a cursor for now We will then ask the cursor for the results , and when that happens , follow the query execution plan and index scan , bit scan and fetch the results and return the results i asked forGetting Started with Cursors Cursors have to work with “ Transactions”postgres=#begin;BEGINpostgres=#declare c cursor for select * id from grades where g between 90 and 100;DECLARE CURSOR When we execute that the query did not actually get executed it just created a plan Now to execute the cursor postgres=#fetch c; Now with each execution of the cursor “C “ will return exactly 1 row from the resultid-121(row) we can keep running the query again and again and the result will keep coming If we want the last record from the result we can run commandpostgres=#fetch last c;Advantages of CURSOR ? Saves Memory on client side (ie open a cursor and then process and results at the pace the backend can receive or transmit ie like process the first 100 and then discard them and fetch the next 100 )eg : for sorting 1 billion rows, if they are coming from db to the application instead of working on it at once , as the data is coming take a batch of it at a time and process and clear of the memory and then receive more data Streaming as we pull in rows to a socket connection or grpc connection continue pulling values .If you are going to access only some of the data in the result set, or access the data just a few times, a server-side cursor minimizes network traffic Easy to Cancel cursors are easy to cancel as when we run a query , in case we requested 1 million rows but want to stop after 100 queries cursor makes it easy to cancel . Paging , not easy to page application with cursor PL/SQL can be used to use cursors Positioned updates Server-side cursors support direct positioned updates, whereas ODBC simulates positioned cursor updates by generating an SQL search and update statement. Direct positioned updates are not only faster, they avoid the risk of unintended update collisions. Additional cursor types With server-side cursors, both keyset and dynamic cursors are available.Cons of Cursor Stateful : It means there is memory allocated for it in the database . and there is a corresponding transaction pointing to that cursor , if made request to another server that does not know that the cursor exists , cannot share cursor (property of transaction) Long Transactions : If we have to iterate through a cursor we have to do it via transaction , and transaction runs for long times , and if transactions are running on database cannot do DDL or index on the database at that time , even certain write operations can be stopped if transaction has acquired some shared lockClient-Side Cursors Versus Server-Side Cursors Every cursor uses temporary resources to hold its data These resources can be memory, a disk paging file, temporary disk files, or even temporary storage in the database. The cursor is called a client-side cursor when these resources are located on the client machine The cursor is called a server-side cursor when these resources are located on the server machine.Client Side CursorWith a non-keyset client-side cursor, the server sends the entire result set across the network to the client machine. The client machine provides and manages the temporary resources needed by the cursor and result set. The client-side application can browse through the entire result set to determine which rows it requires.Static and keyset-driven client-side cursors may place a significant load on your workstation if they include too many rows. While all of the cursor libraries are capable of building cursors with thousands of rows, applications designed to fetch such large rowsets may perform poorly , There are exceptions, of course. For some applications, a large client-side cursor may be perfectly appropriate and performance may not be an issue.One obvious benefit of the client-side cursor is quick response. After the result set has been downloaded to the client machine, browsing through the rows is very fast. Your application is generally more scalable with client-side cursors because the cursor’s resource requirements are placed on each separate client and not on the server.Server-Side CursorsWith a server-side cursor, the server manages the result set using resources provided by the server machine. The server-side cursor returns only the requested data over the network. This type of cursor can sometimes provide better performance than the client-side cursor, especially in situations where excessive network traffic is a problem.Server-side cursors also permit more than one operation on the connection. That is, once you create the cursor, you can use the same connection to make changes to the rows — without having to establish an additional connection to handle the underlying update queries.However, it’s important to point out that a server-side cursor is — at least temporarily — consuming precious server resources for every active client. You must plan accordingly to ensure that your server hardware is capable of managing all of the server-side cursors requested by active clients. Also, a server-side cursor can be slow because it provides only single row access — there is no batch cursor available.Server-side cursors are useful when inserting, updating, or deleting records. With server-side cursors, you can have multiple active statements on the same connection. With SQL Server, you can have pending results in multiple statement handles.Reference https://youtu.be/C1Y6P6vDFts Client Side vs Server Side Cursors " }, { "title": "Cost of Hash Tables", "url": "/posts/Cost-of-Hash-Tables/", "categories": "Backend Engineering", "tags": "Backend Development, Hashing", "date": "2022-06-02 02:30:00 +0800", "snippet": "Hash Tables are effective in many places like Caching Database Joins Partitioning Distributed Databases Sets Load BalancingLimitations and Costs In software engineering, we get away with no...", "content": "Hash Tables are effective in many places like Caching Database Joins Partitioning Distributed Databases Sets Load BalancingLimitations and Costs In software engineering, we get away with not knowing how something works until it breaks and we have to deep dive into its working To understand what hash tables need to start with an understanding of what arrays are The concept of Arrays is a consecutive block of memory Because of the continuous block of memory, no matter how long the index is we can go any node of the element in almost constant time, Big(O) due to being byte-addressable If we ask the CPU to fetch the value of an index, it fetches by using the memory address of the first index of that array and adding the required index X size of each element to get the memory address of that element, Hash Table can be thought of in the same family as the arrays (kind of glorified arrays)CPU Fetching the Results CPU fetching the results from the memory, depends on factors like the distance between the CPU and where the memory is located which is easy to calculate in single CPU architecture, both in Multi CPU architecture like multiple CPU on the boards accessing the single memory Each CPU has to compete to get the access to the memory as a single CPU gets access to the memory at the time to perform an action In some architecture, there are different memory for each CPU allocated, and each CPU accesses its local memory, which is faster and more efficient But there might be cases the CPU needs to access the data in the memory that is local to the other CPU (Non-Uniform Memory access) which can be slow To overcome this limitation by bringing the memory closer to the CPU apple has implemented a unified memory concept in the Apple Silicon by integrating the CPU and Memory together on a single chip with M1 max with both the CPU placed together that the cost of 1 CPU accessing the memory local to the other CPU is negligibleIf Arrays are there why do we need Hash? The problem with the array is that it is integer based, so cannot be used in cases where the key which we want cannot be used as an index To solve these cases is where Hashing comes into the picture to utilize the array in an efficient way Ex : Consider a case when we want to store the mobile number of the user to the name of the user, the 10 digits mobile number is greater than what we can use as the index of an array since the index of the array are integers and the memory required can exceed to that of available physical memory or the index is the name and we want the address for it Sol: We Build the Hash Table, ie take a hash function and generate the Hash for each of the IDs (if we hash the same value again we get the same hash again ) Now give that value, we calculate that index that points to that array, the easier method to do that is using the Module function When we get the hash value we modulo it to the size of the array or index and we can immediately go to that array value in that index and fetch that value Now given a key we can find the value in the hashtable using the power of the arrayCollision in Hash There can be a situation where two students have the same name when generating their hashes the hash will be the same but since the students are different they will have different student IDs that we want to lookup How are collisions avoided in hashmap ? (Java 8 vs Java 11)Uses of Hashes One Popular use case of hashes is in the database join, when we join two tables based on the foreign key we use the concept of the Hash table Supposer we want to join two tables the company table and employee table on of the relation to do that we internally create a hash table (which incurs a cost) so while creating join we pick the smaller table as creating a hash is expensive since we have to loop on every row, take the value and generate the index and store the index in a hash table While generating the hash table we already know the size of the table therefore we can already ask the RAM to allocate us the memory for the hashtable/index which we will be needing and then we start filling the hash table When we have built the hash table we have an array in memory with all those values but there is a mapping between the key to the index value where the data is stored Now we go the other relation, iterate through each of value, hashing each entry and generate the hash for the same and look up the index using the hash table and bring the values corresponding to it (probing continues) If we find the value during the probing and the value match we give the output, if it does not exist we do not join it and create an output relation as a result The above example was classical hash join and there are more optimized Joining hash techniques being used in DB because if the table is big cannot be fixed all in the memory Hash tables are useless if we cannot load them into the memory, as Hashtable is internally an array that needs to be fit into memory to use the byte addressability of the RAMLimitations of Hash Tables Hash Tables have to be fit into the memory to be utilized and that is a big problem, like thinking of using the entire key. value or database into the hashtable 😂😂 , even to build that thing the entire thing would need to be scanned to create a hashtable out of it and load into the memory which is costly ie Cannot have Billion size hash table Building Hash Table has a cost therefore always use the smallest column to choose from for building joins, one can partition it into chunks and hash chunk at a time Deleting and Adding to the Hash table is hard and screws up everything, ex (if we are doing modulo of 10 to all the values, but if we add another element and make it to 11 then we will have to hash the entire thing to make it work with modulo 11) and will have to rehash the entire thing to work with it If we are using Hashing in Sharding of that database where we are using a hashtable to identify which server to point at for incoming request but if the number of server of increase or decrease the whole the whole hashtable has to be created again to work with the new change size (ie DB like Cassandra can handle these problems by re-sharding) Consistent hashing was developed to solve this problemReference https://youtu.be/hxdT_QgHUSg" }, { "title": "Understanding Aggregate Functions Performance", "url": "/posts/Aggregate-Functions-Performance/", "categories": "Database Engineering", "tags": "Backend Development, Database", "date": "2022-06-01 02:30:00 +0800", "snippet": "Understanding Aggregate Functions PerformanceWhat are Aggregate functions?Functions like AVG(A), COUNT(*), MAX (A), and MIN(A) used in DB are useful but if used in the wrong way they can show visib...", "content": "Understanding Aggregate Functions PerformanceWhat are Aggregate functions?Functions like AVG(A), COUNT(*), MAX (A), and MIN(A) used in DB are useful but if used in the wrong way they can show visible performance aggregationIf asked which of the following is more likely to use the index, the answer will be MAX(A), but why and not count ()? , as by initial thinking the COUNT() seems more likely to be using the index to calculate the total count of the rowsAnalysis: COUNT(*)Select count(*) from T; To count every single entry in the table have to scan the entire table Scanning the table would have been easier if it was stored sequentially Scanning the B-tree sequentially is not easy It is easier to read B-tree page by pageThe first thought which comes to our head is that why shouldn’t the index keep a count of the values in the table, why are we not doing that? If we started doing that we will just kill concurrency Counting is one of the hardest problems in the database systemsWe assume that there is a single reader-writer in the system when we are using it but in actuality, there are multiple read writers in a database and currency and there are multiple read and write happening in the database completely unaware of each other, inserting new records at the endNow if we introduce a counter in an index of a DB or a data structure through some sort of mutex or a lock, and whenever a new CRUD operation comes in have to update the count If we Start keeping the count we lose the throughputDatabases are still fast even if we use count just this Aggregate function does not use the index for its work, although there is some exception if we are using the where clause and that uses a field that is indexed and has a small collective number, that will use the index in that case but if we have an unbounded count query it will not use thatThis like these should be thought of and there is no one answer and need to be analyzed all the timeAnalysis: AVG(A)select AVG(*) from T;AVG function is something we need the whole count of same as the COUNT(*) but also iterate through each of those entries to calculate the thing, the moment we start iterating through each value we are going to the DB to fetch the value and less likely to use the IndexException. : Clustered vs Non Clustered IndexAnalysis: MAX(A)Select MAX(A) from T;MAX is the best we can use for the index if we look at an index in which are B Tree are structured and organized into pages and B+tree is designed to be sequential in order, so if we ask the index we already know the first page of the index and the last page of the indexThis means the first page will have the lowest value or number of entries (smallest entry) and the last page will have the max entry, we can use Big O(1) to get to the first page or the last page at the same time cost, and since we can traverse to the last page of that key we can get the maximum of that page The database might need to check if the entry Is alive in the case of Postgres, by fetching that page to check if the value still exists or somebody didn’t delete it or someone updated it, there is where Heap and other concepts come into the pictureIn a nutshell, MAX() and MIN() are the best when it comes to utilizing the index, and if we don’t have the index the database will suffer iterating through all the entries to find the maximum and minimum value Any IO workload hates randomness be it a database or reading the value from drives, to SSD which is random read and writes Best to do IO operation in sorted and ordered things Therefore sorting is important, if things are sorted it makes the operation and so many things efficient and easier to work withBest Case Scenario B Tree is best if need a page from the tree, but not for traversing sequentially though the entire DB We can also store the data by column, ie to find maximum marks in maths pick the column Maths and store all the values in that, so that when we need the max or avg value in maths just a single read on the page will give that column with no garbage unlike the row-based storage model If we look at column-based storage values, they are best for aggregation as a single read will give a collection of values and nothing else giving a lot of IO (ie instead of getting all the non-required details like the student details name, etc instead of required just the marks in subject maths) This is approach is best utilized in OLAPClustered Index The clustered table is a table that has a primary key The table is ordered around that primary key If we have a value that is an integer so if we are inserting the value 1 it is the page 1 and all the values are inserted into order and the new incoming values are also ordered Oracle calls it an Index Organized Table (the table that is organized around the index) SQL Server calls it (Clustered Index )In this setting, the read is faster due to the sorted and ordering nature but the inserts are mostly costly as before the insert the location must be found out inserting into it, which is not the case in the case non clustered index in which all subsequent inserts are appended into the endClustering Sequential Write If we are doing the sequential writes the last page will be the busiest page As the hundreds of inserts are coming in the last page value or position to insert in will always be fighting off for the mutex to lock it before inserting the value There might such cases where random writes will be faster than the sequential ones (advanced concept to think about) No 2 CPUs can write on the same piece of data at the same time (MPI, CUDA multithreading domain comes into the picture, etc)Primary Key as Random? Using UUID as an example, when we insert we don’t have these locks anymore so our writes will be fast as no 2 writes will have to complete on the same page and each will have their own memory location But for writing, we need to pull the page into the memory and write the value to write to the disk back as with all the random writes many pages will be pulled into the memory and soon memory will start filling up which is not the case when writing to the same page as only 1 page will be needed to be loaded into the memory for writing and the only significant cost will be CPU to get the mutex lock Both the approaches have CPU vs Memory costs linked to each other UUID approach will start fast but once the memory pool is filled it starts slowing down when this happens the checkpoint happens when the DB needs to write everything back to the disk consuming all the resources to flush the data back to the disk, this process is expensive and if this keeps happening again and again as we fill the memory over dirty pages and have to flush them If we just changed 1 bit and wrote the entire page back, there is a better approach to thatSummaryIf we are using the aggregate function and the tables are clustered then it will utilize the index as the index is the table as it as a clustered index, with all the nodes in the B+ treeReference https://youtu.be/L-8_CjV6sH4 Page 151 + in Notes (Gate Notes)" }, { "title": "Setup Multiple Git Accounts on Machine", "url": "/posts/Setting-Multiple-Git-Accounts-on-Single-Machine/", "categories": "Software Development", "tags": "Git, Github", "date": "2022-03-26 02:30:00 +0800", "snippet": "😬 Movie and Show Reference stick longer in the head than textbooks, reference of “Curve 25519” in silicon valleyhttps://youtu.be/zdYJi2snJXI1. Generate SSH Open Terminal. Paste the text...", "content": "😬 Movie and Show Reference stick longer in the head than textbooks, reference of “Curve 25519” in silicon valleyhttps://youtu.be/zdYJi2snJXI1. Generate SSH Open Terminal. Paste the text below, substituting in your GitHub email address. $ ssh-keygen -t ed25519 -C \"your_email@example.com\" Example : ssh-keygen -t ed25519 -C \"harshityadav@outlook.com\" -f \"harshityadav95\" This creates a new SSH key, using the provided email as a label.&gt; Generating public/privatealgorithm key pair. When you’re prompted to “Enter a file in which to save the key,” press Enter. This accepts the default file location. &gt; Enter a file in which to save the key (/Users/*you*/.ssh/id_*algorithm*): *[Press enter]* At the prompt, type a secure passphrase. For more information, see “Working with SSH key passphrases.”, Remember the passphrase the same will be required later for while pushing and pulling the repository &gt; Enter passphrase (empty for no passphrase):[Type a passphrase]&gt; Enter same passphrase again:[Type passphrase again] 2. Adding the new SSH key to the corresponding GitHub accountWe already have the SSH public keys ready, and we will ask our GitHub accounts to trust the keys we have created. This is to get rid of the need for typing in the username and password every time you make a Git push.Copy the public key pbcopy &lt; ~/.ssh/id_rsa.pub and then log in to your personal GitHub account: Go to Settings Select SSH and GPG keys from the menu to the left. Click on New SSH key, provide a suitable title, and paste the key in the box below Click Add key — and you’re done!3. Creating the SSH config fileHere we are actually adding the SSH configuration rules for different hosts, stating which identity file to use for which domain.The SSH config file will be available at ~/.ssh/config. **Edit it if it exists, or else we can just create it.$ cd ~/.ssh/$ touch config // Creates the file if not exists$ code config // Opens the file in VS code, use any editorMake configuration entries for the relevant GitHub accounts similar to the one below in your ~/.ssh/config file:#user1 accountHost github.com-harshityadav95 HostName github.com User git IdentityFile ~/.ssh/harshityadav95 IdentitiesOnly yes#user2 accountHost github.optum.com-hyadavXX HostName github.harshityadav.com User git IdentityFile ~/.ssh/hyadavXX IdentitiesOnly yes“harshityadav95” is the GitHub user id for the work account.“github.com-harshityadav95” is a notation used to differentiate the multiple Git accounts. You can also use “harshityadav95.github.com” notation as well. Make sure you’re consistent with what hostname notation you use. This is relevant when you clone a repository or when you set the remote origin for a local repositoryThe above configuration asks ssh-agent to: Use harshityadav95 as the key **for any Git URL that uses @github.com** Use the hyadavXX key for any Git URL that uses @github.com-hyadavXX$ cd ~/.ssh/$ touch config // Creates the file if not exists$ code config // Opens the file in VS code, use any editor4. What account should be default?Make it global:$git config --global user.name \"Harshit Yadav\"$ git config --global user.email \"my@pers.on.al\"This will be used by default. in case you do not set the local config5. While Cloning RepositoriesNote: step 7 will help, if we have the repository already available on local.Now that the configurations are in place, we can go ahead and clone the corresponding repositories. On cloning, make a note that we use the host names that we used in the SSH config.Repositories can be cloned using the clone command Git provides:git clone git@github.com:personal_account_name/repo_name.gitThe work repository will require a change to be made with this command:git clone git@github.com-harshityadav95:harshityadav95/testingmultiple.gitThis change is made depending on the hostname defined in the SSH config. The string between @ and: should match what we have given in the SSH config file.6. What account should be local?Make it the local to that repository:$git config user.name \"Harshit Yadav\"$git config user.email \"harshityadav@outlook.com\"This will be used by local and visible in commits.7. For Locally Existing RepositoriesIf we have the repository already cloned:List the Git remote of the repository, git remote -vCheck whether the URL matches our GitHub host to be used, or else update the remote origin URL.git remote set-url origin git@github.com-worker_user1:worker_user1/repo_name.gitEnsure the string between @ and : matches the Host we have given in the SSH config.8. If you are creating a new repository on local:Initialize Git in the project folder git init.Create the new repository in the GitHub account and then add it as the Git remote to the local repository.git remote add origin git@github.com-work_user1:work_user1/repo_name.gitEnsure the string between @ and : matches the Host we have given in the SSH config.Push the initial commit to the GitHub repository:git add .git commit -m \"Initial commit\"git push -u origin masterThere are more smaller and cleaner updated way too I came across but were taking longer to grasp the conceptReference https://gist.github.com/Jonalogy/54091c98946cfe4f8cdab2bea79430f9 Developing with multiple GitHub accounts on one MacBook" }, { "title": "What is SSL/TLS ?", "url": "/posts/What-is-TLS-SSL/", "categories": "Networking & Communication", "tags": "Web Development, Network", "date": "2022-03-25 02:30:00 +0800", "snippet": "\"SSL is the equivalent of arranging an armored car to deliver credit card information from someone living in a cardboard box to someone living on a park bench\"Getting StartedTransport Layer Securit...", "content": "\"SSL is the equivalent of arranging an armored car to deliver credit card information from someone living in a cardboard box to someone living on a park bench\"Getting StartedTransport Layer Security, the successor of the now-deprecated Secure Sockets Layer, is a cryptographic protocol designed to provide communications security over a computer network.So to clear up the difference between SSL and TLS with ELI5 found this post on Hackernews Could you please ELI5 the difference between SSL and TLS? My office is also moving to tlsTLS1.2 and the communication seemed to use both SSL and TLS interchangeably. The Windows registry has entrees for both SSL and TLS. I am confused. ? There is no difference. SSL was invented by Netscape, when they brought it to IETF (Internet Engineering Task Force) to get standardized Microsoft did not want to use the name Netscape has advertised, so they forced a name change. Thus “SSL 3.1” (IETF update to SSL 3) was called “TLS 1.0”. This was in 1999.TLS 1.1 uses a version encoded in two bytes that should mean “SSL 3.2”, TLS 1.2 uses bytes that should mean “SSL 3.3”. TLS 1.3 pretends to be TLS 1.2 to pass through proxies but internally uses two bytes that should mean SSL 3.4 to indicate its version. In short, TECHNICALLY the only thing that exists is TLS. The implementation history is SSL 3.0 &lt; TLS 1.0 … 1.3However, SSL is still used colloquially in conversation, e.g. “SSL certificate” and in many legacy config flags For best clarity, check the details of all those settings, even the SSL ones. But TLS is the term for modern standards.Why do we need a Certificate?Video Explanation: What are SSL/TLS Certificates? Why do we need them? Work? Suppose I want to visit ABC.XYZ and want to consume content from the server I will send the GET request to the server Since the request can be intercepted by anyone in the middle, therefore, we don’t send the request in plain text therefore we encrypt it.There are two types of Encryption - Symmetric, and Asymmetric So how do we generate a key to lock the request data which when reaching the server, the server also has the key to unlock the requested data The first thought will be to send the key first before sending in the data but if we send the key first then if there is someone in the middle intercepting the message then the key will also be intercepted and used to unlock and read the message by the person in the middle Thus we came up with the idea of Public Key EncryptionAsymmetric Encryption Public Key Encryption: Server has 2 Keys RED (Private)key and BLUE (public) key \\ If Public Key is used for Encryption the Private Key can Decrypt Can also use Private key to Encrypt and Public key to also Decrypt The Client will generate a new GREEN Key to use for Encryption and we need to send it to the server without being read in between by any middle man intercepting the trafficKey Exchange (OLD Method) The Client Says it wants to establish a connection with the server, so it sends a request to the server asking “ want to establish connection “ The Server will reply back here is my BLUE Key (Public Key) take it Now we as clients will Encrypt the Green key with BLUE (Public Key of the Server) and send it to the server which can only be decrypted by the RED Private key of the Server From then on all the communication between client and server will be encrypted using the GREEN KeyProblems with Key Exchange (OLD Method) What if the message got intercepted in between and when we’re asking the server for its public key by a Middleman called Thanos and he sends his public key pretending to be youtube.com server whose public key we wanted And we accept Thanos Public key and start communicating with it thinking of it as a youtube server So how to verify that the Server is Youtube one Thanos One ?, we need a certificate along with the response that we can verify is coming from the genuine Youtube Server and not the Thanos Server (Answer Meet Certificate Authority)Where are HTTPS and HTTP in this Picture? Let’s see we have a client-side server (Port 443) and server (www.google.com) , set up a TCP Connection (Stateful) now to get data we create a TLS handshake to share data between client and server without anyone in the middle reading the data (sharing the same key across to exchange data) HTTPS is one of the many applications of SSL/TLS: when you interact with an HTTPS (S for ‘secure’) website like https://harshityadav.in, it is SSL/TLS that secures the web-traffic (made of HTTP interactions) between your browser and the website. Client Sends Data encrypted using the shared key and the server decrypts it using the shared key The server then Sends back the data to the client again in encrypted form using the key which can be decrypted by the clientHow TLS Relates to TCP/IP Protocol Stack As illustrated by the yellow box in the figure, it sits between application layer protocols such as HTTP, SMTP, FTP, NNTP, LDAP, and POP, and the transport protocol TCP. Sitting in the middle of the two layers, TLS secures application layer data transmitted over TCP. For instance, in HTTPS, TLS secures HTTP traffic transmitted over TCP. DTLS — a sibling of TLS — is to UDP what TLS is to TCP.Message Flow in Full Handshake ProtocolVideo Explanation : YoutubeWhat is Certificate Authority?Video Explanation: Certificates and Certificate Authority Explained How as a client to make sure that you are actually communicating and connecting to the original www.google.com server instead of someone acting as a google server by intercepting the request in the middle and pretending to be google and sniffing through your data and giving the results it fetches from google by establishing a connection with google.com (You ←——&gt; Thanos ←———→ google.com ) Thanos Intercepting like Man in Middle To solve the problem of verifying that the person we are communicating to is Not Thanos in the Middle we introduce Certificate Authority which proves the identity of google.com and everyone trusts the Authority Certificate Authority Like IdenTrust, DigiCert, Sectigo, Lets Encrypt, GoDaddy So when establishing a Server for the First time say google.com talks to the certificate authority (ex Lets’ Encrypt), telling the address details (like getting a passport, in this case, public Key of Google ) which when are encrypted by the private key of the certificate authority is called the signed certificate which is also previously signed by a root authorityHow Does Certificate Work in General? Let’s Suppose I am the client and want to establish a TLS connection between the server and Server and Client we do a handshake In Handshake, we exchange the public key of the server and the server sends the certificate The certificate has some information signed by the “ Certificate Authority “ which is in turn signed by “ root certificate “ which is in turn self-signed (Chain of Trust) ie so when talking to google.com we receive a certificate from google signed by (Let’s Encrypt ) verifying google identity and we trust the (Let’s Encrypt Claim) since it was signed by Root Certificate which we already have on client-side pre-installed Once the TLS Certificate reach the client, the client validates the certificate (the Certificate in the Browser or OS does the validation)How does Certificate Validation Works The client takes a look at the certificate and checks if encrypted and signed by which certificate authority but does the client trust the certificate authority? who signed the certificate authority, the root certificate (which is self-signed) so how to validate who signed the root certificate? Root Certificate is pre-installed in the client (mobile device, computer) These Root Certificates are trusted globally Hard to fake a signature since no one else has the private key of the certificate authorityNow Consider the Scenario Someone taps in the middle and intercepted using techniques like DNS poisoning, terminating the traffic the communication and served a shady clone of the website (from the stolen code of the website )with same API and interface and (like serving aa.com instead of ab.com ) which is intruders own website that has a certificate signed by self-signed or certificate authority and root certificate that is trusted by the end-user (client) If this happens then our laptop will validate the website and accept the (aa.com instead of abc.com ) as the genuine website or even ab.com if the attack was done using DNS poisoning The attacker will terminate the connection in the middle and intercept the traffic on behalf of the original server and can steal the dataNote :If Super paranoid check the certificate authority of the page when connected to public wifi or WorkMachineSSL/TLS Certificate Pinning More Concerned with Frontend Application and focusing and delivering to the end-user (What about how we secure and establish trust between backend Applications) The attacker can always easily install a wrong certificate in the client device in the device like android that will make the client trust the incoming certificate from a malicious website The solution, we hash the certificate along with the URL it points to and stores it locally When we visit the website ab.com we verify the certificate locally using the methods listed above as the first layer but further checks the hashed certificate stored in the application layer to match the authenticity if it does not match we fail the connectionWhat are the Cases when the Certificate don’t match? If someone is intercepting in the middle the certificate sent by them won’t match, ie the certificate validation will pass but at runtime when the code kicks in the hashed certificate won’t match thus detecting man the middle attackAdvantage Can use self-signed certificate not using any certificate authority, hash it and store it in the application, the SSL will accept it but at the code level, it can be rejected. or not by using — ignore (not recommended but still people use it) If using a gaming app one knows it will be connecting the one server where certificate pinning makes sense, but does not make sense to implement it in browser applications as the browser should be able to connect to any web server It is only useful in use cases where one knows that the application has to connect with only a limited range of servers and nothing else.Disadvantage If the certificates expired or the certificate has been revoked, then the certificate has to be rehashed and updated at the end client application. If not the client will still be trusting the old certificate. if the update does not reach the end clientThese may seem like limitations but there are good practices to it that can be done at the application layer to implement this make better and more secure mobile applicationCredit for ExplanationHussein Nasser, Youtube Channel , http://www.husseinnasser.com , twitter: @hnasrReference SSL and TLS with ELI5 found this post on Hackernews eli5_what_are_online_security_certificates_ssl/ TLS/SSL Certificate Pinning Explained Making Sense of SSL/TLS" }, { "title": "Books I Read in 2021", "url": "/posts/Books-I-Read-in-2021/", "categories": "Books", "tags": "Reading, Year Goals", "date": "2021-12-31 02:30:00 +0800", "snippet": "With the Start of the Year 2021, the number of books I planned to finish in both technical and general reading was much higher but by the end of the year I Still managed to Complete three Books, wh...", "content": "With the Start of the Year 2021, the number of books I planned to finish in both technical and general reading was much higher but by the end of the year I Still managed to Complete three Books, which it would have been great to summarise over here but among these 3 books will be revisiting 2 of them so will probably write about 2 of them later.Book 1 : Almanack of Naval RavikantThe Book is Completely open and Freely available across all platforms in both text as well as podcast format.Source : https://www.navalmanack.com/Book 2: Ai Superpowers: China, Silicon Valley, And The New World OrderBeing in India and working in IT, since school days we know a lot more about Silicon Valley in the USA but so less about the tech industry in our neighboring country China, it is much more than Ai and how china is moving ahead at warp speed in all domains and distinct work culture and market difference in APAC and US regionsSource : Amazon LinkBook 3: Atomic Habits: An Easy &amp; Proven Way to Build Good Habits &amp; Break Bad OnesThe book is Bestseller in every platform and in majority of recommendations list this year really liked the supporting reason and short story at the start of each chapterSource : https://jamesclear.com/atomic-habitsPS: At the time of writing this article have started with Book Clean Code, and more to come" }, { "title": "Completing AZ 900", "url": "/posts/Microsoft-AZ-900.md/", "categories": "Cloud", "tags": "Azure, webdevelopment", "date": "2021-12-08 02:30:00 +0800", "snippet": "Completing Azure CertificationSo Completing the first Cloud Certification in Azure , to know more about exam watch first 15 min of this VideoHow to PrepareFirstly, seeing the term exam has still go...", "content": "Completing Azure CertificationSo Completing the first Cloud Certification in Azure , to know more about exam watch first 15 min of this VideoHow to PrepareFirstly, seeing the term exam has still got its weight coming out of college but the exam was quite easier, here is how I would recommend someone beginner to go with AZ 900,1. GO PracticalNo Amount of theory can beat practical hands so just don’t dive into the tutorial use a free trial account and try to achieve the following tasks with articles and not videos if possible and just try to explore, Create a Virtual Machine of Windows and Connect to it Create a Database Instance for MySQL and do CRUD operation from either python or Java Create a Static webpage and host it on Azure2. GO TheoreticalPart of clearing the exam is getting familiar with the terminology used in Azure, so even if you have prior experience with AWS or Gcloud or have worked for years with Azure having a go through the reading of the official microsoft az900 study material is all that is needed, no udemy, youtube , Coursera videos needed for preparation Go through all 6 Chapters and their submodules in a week (Module 2-3 are have more submodules so keep motivation levels up) Make Short notes of terms, and concepts in your diagram and connection understanding for later revision of googling unfamiliar terms3. Practise QuestionsSo just a day before the exam practise through some online sample questions set there are many online just going through the first 50 questions will give you complete picture as all the rest questions are around the same topic and concept so small question set will be enough, the site I used is listed below, the syllabus keeps changing from time to time but not much so if you come around some questions not covered in theory study material do not worry about that portion Exam Sample Questions4. Taking the ExamThe Exam itself is easy with plenty of time to attempt questions and think the number of questions is different based on question type for me it was 41 questions to be completed in 45 minutes, was able to complete in less than 30 minutes , read the question properly and think about all the options before selecting5. After Exam what Next?After completing the exam the score is available immediately so if it’s above 700 you know you have passed, and the certificate takes about 30 min to get generated and receive mail regarding it, there are more levels for Azure exams but those need to be renewed every 18 months, AZ 900 being a fundamental exam does not have an expiry dateSo here is my Exam Certificate and Credentials : Credly Link Badge" }, { "title": "Spring Boot Notes", "url": "/posts/Spring-Boot-Notes/", "categories": "Java", "tags": "SpringBoot, webdevelopment", "date": "2021-10-12 02:30:00 +0800", "snippet": "Spring BootBefore Spring Java Beans, POJO Lacked in Security and Transaction Management Then came enterprise java beans EJB Solved Transaction processing Session Processing E...", "content": "Spring BootBefore Spring Java Beans, POJO Lacked in Security and Transaction Management Then came enterprise java beans EJB Solved Transaction processing Session Processing EJB development was not easy Spring Emerged as answer Intro to SpringSpring is an open-source lightweight framework , main technologies used by Spring Framework for developing enterprise applications Aspect-Oriented Programming AOP Enables cross-cutting concerns in a centralized area POJO Dependency Injection DI Java enterprise edition specification Spring Framework Relies heavily on Inversion of Control Build using POJO - plain old java object Spring works with java Standard edition POJO+ configuration Metadata = Spring container A Spring bean is a basic building block that is managed by Spring Framework Spring is responsible for creating and destroying beans Providing dependencies of the bean which could be other beans or configuration properties Intercepting Bean method calls to include additional framework featuresWhat makes spring powerful? How it manages dependencies Design patterns are the responsibility of developers , solution Inversion of Control ie role of managing dependencies is handled over to Spring frameworkWhat is Inversion of Control (IoC)?A process by which objects define their dependencies and an external container injects those dependencies into the object , the object need to worry about where its dependencies are coming fromIt is also called dependency injection where we let the spring container instantiate objectsSpring FeaturesIoC Container (Inversion of Control) It is a process by which an object defines its dependency (ie the other object they work with either through constructor argument or simple property ) The Container Injects the dependency when the bean is being created It is a core container that uses dependency injection to implicitly instantiate objects during runtime This container also handles configuration management of application objects The process is Inverse therefore the inversion of containerSpring MVC Framework Create web applications using MVC architecture All requests made by the user goes through a controller and gets dispatched to model or view based on the mapping This framework can be integrated with all frontend technologiesData Access Framework It allows the developer to use persistence API such as JDBC or hibernate to store or access data in the database Interacting with database, connection, closing the connection ie all the keys concerning an interaction with the database or exception handling along with transaction management can be handled easily with data access frameworkTransaction Mangement Provides Java Transaction API , JDA for global and local transaction Create a wide range of transactions on basis of Spring declaration transaction managementSpring web service A powerful mechanism for distributing messages between two machines It generates web service endpoints based on java classes providedJDBC abstraction layer Handles error in an easy and efficient way Reduce the JDBC programmingSpring TestContext Framework Do unit and integration testing provide key integration testing functionalities such as Context Management Caching Dependency Injection of the test fixture Supports transactional test management Default Rollback SemanticsSpring Code Module Core component of spring framework provides the IoC container there are two types of implementation of spring containers Bean Factory It acts as a single IoC Container for instantiation application objects It also configures and assembles dependencies between these objects Application Context It provides a central configuration for an application Spring ORM Module Used for accessing data from databases in an application It provides supports for various ORM frameworks like Hibernate Simple declarative transaction management, resource management, and transparent exception handlingSpring AOP Modules Object Oriented programming breaks the program into a hierarchy of objects Whereas AOP break it into Aspects or concerns They are typically denoted with aspect annotation Aspect Helps developer implement cross-cutting concerns in a centralized fashion , rather than implementing similar objects in multiple placesSpring MVC Modules Implements MVC design request from the browser goes to dispatcher servlet which then sends it to a controller based on a set of Handler mapping the Controller process the application and returns the response to the dispatcher servlet in form of the model object Dispatcher servlet uses view resolver to send back the responseSpring WEB Flow Module It is an extension to the spring MVC model It helps in defining flow between the different user interfaces in the application Helps virtually split up an application into different modules and use them accordinglySpring DAO Module It introduces JDBC abstraction layer by eliminating the need for boilerplate coding it supports a programmatic and declarative approach to transaction management Easier support to access the database resourcesSpring Application Context Module This is based on the core module of the spring framework , it supports the features from the bean factory and other features such as: Internationalization Validation Resource Loading Implements message source Interface and provides messaging functionality to an applicationModel View Controller Design Pattern Data access layer Service Layer Presentation LayerSpring Boot Inversion of Control Dependency InjectionSpring Framework Modules1. Core Container The core container is responsible for managing beans Sets up the context of the application Special Expression language “SpEL” All other projects are built on top of the core container Spring-core and spring-beans for IoC and dependency injection Application Context eliminates singletons and decouple components spring-context for access to objects in JNDI (Java Naming and Directory Interface) registry style spring-expression for working with the object at runtime2. Data Access and Integration spring-JDBC abstracts away vendor-specific error codes and handling spring-orm for working with Java-Persistence API (JPA) , Hibernate, and other ORM API’s spring-JMS, spring messaging for message processing spring-tx for working with POJO declaratively3.Messaging spring-messaging module Message, MessageChannel and MessageHandler abstraction Annotations for mapping messages to methods Similar to Spring MVC annotation based programming4. Web spring-web , spring-web MVC, and spring-WebSocket modules spring-web for basic web features eg, servelet listeners , HTTP client spring-web MVC for web application programming using MVC paradigm spring-WebSockets as a thin lightweight layer above TCP(AOP) Aspect Oriented Programming Programming paradigm that adds new “ aspects” to the behavior of existing code using “pointcuts” (external specifications) This ensures existing code is not modified to add new behaviorAOP in Spring AOP alliance-compliant aspect-oriented programming Add additional functionality using interceptors , pointcuts, and source-level metadata Implement aspects with @Aspect annotation Spring AOP modules helps combine OOP with AOP5. Testing Unit-testing as well as integration testing Junit or TestNG Loading and caching of Application Context objects Mock objects to test code in isolationDependency ManagementDependency management specify what JAR (java archives ) and libraries our project depends upon process of correctly getting all required jar files into the correct location (and into classpath) so that spring works correctly Extremely important and somewhat tricky to get right Dependencies include compiling time as well as run-time Different and distinct from dependency injection Deals with physical resources (files) Direct vs transitive dependencies Transitive dependencies are hardest to manage Need a copy of all jar libraries for Spring Separated into modules, use what is needed Spring publishes artifacts to Maven Central Maven Central can be thought of as a repository for JAR files Also publishes to specific public Maven repo Either use Maven, Gradle, or Ivy Install manually or use any above tool aboveModel View Controller Spring MVC or Spring Web MVC, model-view-controller paradigm for building web appsModelModels contain the application data as POJO Encapsulates application state (but not application logic) Can be queried to obtain state Notified by the controller when the state needs to change Notifies controller once the state has been changedControllerHandles the user request and act as a router between the model and the view Defines application logic (not application state) Maps user actions to state changes Updates application state only via model (not directly) Updates views once application state has changedViewsRenders the output Data from the model in a presentable format Present application state to users via an appropriate interface Allow users to interact with the state, modify the state Do not store application data (except for caching) Built using reusable and configurable elementEach component when updated notify the listener via Synchronous methods vs. Asynchronous eventsSpring MVC FrameworkBased on Model, View Controller Design PatternSpring framework relies on three underlying technologies, along with the web features, it also supports the core functionality of spring frameworks such as IoC and dependency injection Model - Model is typically your application data, it can be a single or a collection of objects Controller - acts as a router between model and view and it contains all the business tools of the application, any class marked with @Controller annotation will make it a controller class View - it represents the information in a particular format, JSP is an example of creating a view page Front controller - DispatcherServlet works as a Front controller and it is responsible for managing the flow of the Spring MVC application one of the key components of Spring MVC is the DispatcherServlet and it is a class that receive all the incoming requests and maps it accordingly to the model and the controllers Spring MVC Execution Flow Dispatcher Servlet receives the incoming request Dispatcher servlet identifies the controller class based on the handler mappings Once the controller has been identified the request gets processed After processing it returns the model data and the exact view name Dispatcher Servlet sends the model data to the view resolver to determine the view page Dispatcher Servlet returns the view pageAdvantages of Spring MVC Separate roles - Spring MVC defines separate roles for the different components Powerful configuration - It provides robust configuration support for context references, web controllers, business objects, and validators. Light Weight - Uses lightweight servlet container to develop to deploy your application with all the easy configuration and reduced boilerplate coding Rapid development Ease to Test - by injecting the data using the setter methods Reusable business code - instead of creating new logic, use existing business objects Flexible Mapping - with help of annotations, mapping the configurations, redirections are very straightforwardDisadvantages of Spring MVC Steep learning curve Framework version instability Dependency injection (entire project dependent on spring framework)Components Servlets JSP (Java Server Pages) JSTL (Java Pages standard tag library)Servlets –&gt; JSP –&gt; JSTL (collection of useful JSP tags for common tasks)Servlets used to build dynamic web pages in java existed since 1996, when most web pages were static Run on a Java-enabled web or app server Handle incoming HTTP requests Servlet (Java program) runs within the environment of Servlet Engine Servlet Engine also referred to as Servlet Container Deals with cookies and MIME types Support for session management and securityJSP , Java Server Pages Java ServerPages is an abstraction on top of servlets JSP scriptlet is a basic unit -enclosed in tags &lt;% ……%&gt; JSP scriptlet will be injected into a servlet at runtime Servlet corresponding to JSP scriptlet will be cached and re-used until JSP is modified JSP Compiler is needed to compile JSP scriptlets to servlet JSP compiler runs on java-enabled web app or server Servlet code runs inside JVM on the webserver JSP allows Java code and HTML to be interleavedJSP Standard Tag Library aka JSTL Sciptlet tags are basic building blocks of JSP JSTL is a standard library for JSP tags Taglibs contains the core functionality of JSTL Taglibs ship with every servlet and JSP frameworkJSTL Tag Classification JSTL Core : loops , control flow ,&lt;div&gt; output JSTL Formatting : dates, internationalization JSTL SQL : use usually discouraged (security) JSTL XML : working with XML documents JSTL functions : mainly string manipulationModel View Controller Dispatcher Servlet as a front controller to receive browser requests Dispatched to the appropriate controller via handler mappings Result returned to the user via ViewResolver ClassesTypical Spring MVC App Model - Pojo (Plain old java object) Views - JSP templates written with JSTL Controller - Dispatcher Servlet useful for classic 3 tier architectureREST based ControllerREST (Representational State Transfer) is an architectural style that uses HTTP resources to create a REST web service@RestController Used to create controller component Serve as an entry point for handler mappings@ResponseBody Return value for the HTTP response object Spring will convert the return value to HTTP response based on the content type in the header, like JSON One of the key classes that represent the entire HTTP response is response Entity All return types along with corresponding response code can be customized with this class@RequestBody It is similar to the response body Spring binds the incoming HTTP request to the annotated parameter Converts the request into domain object based on the content type in the header@PathVariable Denotes the method parameter that will be bound to the resource URLSpring HTTP Methods HTTP GET HTTP PUT HTTP POST HTTP DELETESpring MVC Rest API Advantages Follows MVC architecture but all requests are handled through a controller Enables sepration of logical component for easier maintainence Dedicated Annotation allows defining configurations easily Bypass view based rendering Path variable annotation Resource repsresentation Request body annotation REST template classSpring Data Provides a familiar and consistent programming model for a developer to access the datastore Spring Data reduce the boilerplate coding Provides powerful repository and custom mapping abstraction Easy Integration with other spring modules Dynamic Query Generation from the method names Support wide range of persistent store Relational Database Non-Relational Database Map-Reduce framework Cloud-based data service Java Persistent API It is an object-relational mapping standard created for Java to store access and manage objects in a relational database Spring Data adds layer on top of the JPA for easier integration with other spring modules JPA reduce boilerplate coding Focus more on business logic than configurationFeatures of JPA Spring data support type-safe queries that enforce data type validation It supports the ability to keep track of who created or changed an entity and the point in time this happened Pagination support, where data from large result sets is returned in chunks based on page size and number Dynamic Query execution using API suggest JPA criteria Support for XML based entity mapping for configuration filesSpring Data LDAP (Lightweight directory access protocol) LDAP directories are hierarchical data stores used for storing user information to support authentication and authorization used java-based classes or XML namespace Annotation based mapping metadata Automatic implementation of repository interfaces Support QueryDSL IntegrationSpring Data Elasticsearch Uses POJO for interacting with an elastic search model Elasticsearch documents Repository access layer XML based, object mapping integrated with spring conversion services Automatic implementation of repository interfaces Context and dependency injection support while interacting with repositoriesSpring for Apache Hadoop Spring provides a unified configuration model and easy to use API Enabling integration with other spring objects enabling the developers to building solutions for big data HDFS is a distributed file system that provides high-performance access to all data across the Hadoop clusters MapReduce is a framework supported by Hadoop for processing huge amounts of data in parallel Apache Pig is a platform that analyzes large sets of data that consists of a high-level language for expressing data analytics programs Apache Hive is a data warehouse for providing data query and analysis, it provides a SQL like an interface for querying the data stored across different file systems and databaseSpring Security It is a Java Enterprise edition framework for securing enterprise Spring-based application Provides features for both authentication and authorization Servlet API Integration Web Attack prevention (session fixation,clickjacking,cross-site request forgery ) Supports Integration with Spring MVCAuthentication Mechanism to verify or establish your identity Common method to do so is using user credentials Various forms of Authentication Single Factor Authentication - requires password Two Factor authentication - password + other info Multi Factor Authentication - password+ info+ passcode Authorization It is the process to determine whether the authenticated user has access to a particular resource or not Access control for URL Secure object and methods Access control listsSpring Security OAuth OAuth Providers OAuth consumer Supports OAuth1(a) Support for OAuth 2.0Spring Security SAML Security Assertion Markup Language Authentication and federation mechanism in a single application Supports SAML 2.0 which uses security tokens containing assertions to pass the principal user information between the service provider and identity provider IDP (identity provider )and SP (service provider)single sign-on Service provider metadata generationSpring Bean Spring bean is a simple object that is instantiated, Assembled, and Managed by the IoC container The Container Injects the dependency when the bean is being created The process is Inverse therefore the inversion of container Bean Factory interface is the central IoC interface in spring It is the actual representation of the spring IoC container responsible for managing the beans The most commonly used BeanFactory interface is bean factory class using XML Spring configuration contains at least one configuration that the container must manage There will be more than one bean definition with the XML configurations and these beans are configured as elements in the bean tag Configuration metadata informs the spring container on how to instantiate, configure and assemble objects within your application The configuration metadata is maintained in XML format Instantiating Spring IoC container is straightforward can be achieved with the use of application context Spring also supports annotation-based configuration for bean creationSpring Bean Definitions Bean Definitions are represented as bean definition objects which contains a qualified package name, which is the actual implementation class of the bean being defined Behavioural configuration elements which states how the bean should behave in the container References to other Beans which are needed for the beans to its work, these references are also called collaborators or dependencies Other Configuration Settings to set in the newly created object, best example is managing a connection poolSpring Bean Definition InheritanceInheritance is ab object-oriented programming mechanism where an object is created or derived from another class which is usually a parent-child relationship A Bean definition contains a large amount of configuration information, including container-specific information , constructor arguments, and property values A child bean definition on the other hand is a bean definition that inherits the configuration data from a parent definition Child bean definitions can overwrite values if needed Bean Factory - child bean definitions are represented by the ChildBeanDefinition class Most Developers configure bean declarations in XMLBeanFactory _**_When using XML-based configuration metadata a child bean definition is indicated simply by using the parent attribute Configuration Information, some of the configuration information for bean definition inheritance are scope, constructor arguments, properties , and over overriding methods ChildBeanDefinitions, A child bean definition will use the bean class from the parent definition if none is specified At the same time, it can also override it Child Beans have to be compatible with the parent It will inherit constructor argument values, property values, and the method override from the parent It also has option to add new values if neededSpring Bean ScopesWhen a bean definition is created we are instantiating a class , Along with dependencies and configuration, Spring allows us to control the scope of the application as wellSpring Support 5 different scopes Singleton Scope- only one object instance will be created for the single bean definitions Prototype Scope- scopes a single bean definition to any number of object instantiations Request Scope - Request scope is valid until the lifecycle of a single HTTP request, each and every HTTP request will have its instance of the bean Session Scope - works throughout the lifecycle of the HTTP session , the best example of session scope is when a user tries to login into an application and the session is valid until the user decides to log out or close the application Global Session - scopes a single bean definition to the lifecycle of a global HTTP session, this is only valid when the web application is under the application contextSpring Boot Auto-configuration @ConditionalOnClass - check dependencies on specific classes before creating auto-configured beans @ConditionalOnProperty - check dependencies on properties before creating auto-configured bean @ConditionalonMissingBean - only create auto-configured bean if no user-specified bean is availableSpring Boot Microservices*Maven Article : POM - Project object Model war file - web archive files&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;in.harshityadav&lt;/groupId&gt; &lt;artifactId&gt;mavenlearner&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;mavenlearner Maven Webapp&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;mavenlearner&lt;/finalName&gt; &lt;/build&gt;&lt;/project&gt;QuestionsWhat is Spring Beans?Objects whose lifecycle is entirely managed by SpringWhich techniques using Spring will achieve inversion of control? Dependency Injection Factory Pattern Server Locator patterWhat is Aspect-oriented programming? Adding functionality to existing code without changing the code itselfWhich dependency management systems can be used with Spring? Gradle Maven IvyModel - Responsible for holding and updating application stateView - Responsible for presenting application state to the userController - Holds the business logic of the applicationWhat is the front controller in Spring MVC called? Dispatcher ServletExercise to Self Learn and Practise : Create a Simple Spring Application using STS Create a Controller using Spring Create a REST Controller with getting Method Create a REST Controller with POST Method Use REST Template to access the endpointQuestionsWhat annotation must be specified for the public class of your application to include Spring Boot? - @SpringBootApplication Dev Tools Cache Disabling Live ReloadQ. What is group id and Artifact ID?Ans : groupId specifies the id of the project group while the artifactId specifies the id of the project.Q. What is aspect orinted programming ?Ans : In a typical enterprise application there are 3 layers UI Layer Business Layer Data Access Layer.Across these threee layers there are some cross cutting functionalitty that are required in all the three layers like logging Profiling security transaction managementIn “ Object oriented programaing “ there is a class and an object is the key unit that represents that class ,in “ Aspect oriented programming “ there is aspect (key unit) or a specialised unit that address the cross cutting concern like onf of the 4 listed cross cutting functionlaity listed Through AOP we can address the concerning cross cutting functionality Reuse : Once we develope Aspect we can use it across enterprise application , Quick Development : focus more on business logic and can add these functionality any point in time Focus on Aspect : one dev can focus and build on one aspect while the other dev can work on another Enabled / Disabled : Enabling and disabling aspects at runtime of the project during the configurationLibraries that use AOP : Spring AspectJQ. What is dependency Injection ?Ans : Dependency is basic things that a code depends upon to run. ( a computer needs a cpu and ram as dependency to run ) , one way to implement this in code is using dependency constructor , With dependency injection , we inject the dependency into the class instead of preparing in the class which needs it No need to use “new” operator inside the class , or object of fancy container Pushing the dependency in the class model and accesing using the constructor parameter or via setterSource : https://www.youtube.com/watch?v=IKD2-MAkXyQ&amp;ab_channel=AnthonyFerraraQ. What is Dependency Inversion Principle ?Ans : Code shouldnt depend on depenendcy rather then its abstraction (Interface) By depending on Abstraction we we are decoupling on ImplementationQ. What is Dependency Injection Container ?Ans. : It is a map of dependency that a class needs with the logic to create those dependency if they haven’t been created yetSo every time we ask for dependency while creating bean The map will figure out which dependency to use Containe will see if it has one of those dependency already , if yes it will use that one If No , create dependency , store it Q. What is Inversion of Control ?Ans : Commonly we write and start our code , and move control to the library logic wherever needed and once that is needed the code execution flow is back in our code , in Ioc the framework starts and has the control flow and call our code whenver is needed and when our code use is over the flow goes back to the frameworkSource : https://youtu.be/oLxsTnH_peIin Spring. : dependency is with the interfaceQ. Configuration, This annotation is the main artifact used by the Java-based Spring configuration; it is itself meta-annotated with @Component, which makes the annotated classes standard beans and as such, also candidates for component-scanning. The main purpose of @Configuration classes is to be sources of bean definitions for the Spring IoC ContaineQ. Autowiring By declaring all the bean dependencies in a Spring configuration file, Spring container can autowire relationships between collaborating beans. This is called Spring bean autowiring. After enabling annotation injection, we can use autowiring on properties, setters, and constructors. Q. Application Context Spring IoC container is responsible for instantiating, wiring, configuring, and managing the entire life cycle of objects. BeanFactory and ApplicationContext represent the Spring IoC Containers. ApplicationContext is the sub-interface of BeanFactory. BeanFactory provides basic functionalities and is recommended to use for lightweight applications like mobile and applets. ApplicationContext provides basic features in addition to enterprise-specific functionalities which are as follows: Publishing events to registered listeners by resolving property files. Methods for accessing application components. Supports Internationalization. Loading File resources in a generic fashion. Reference https://docs.spring.io/spring-framework/docs/2.5.x/reference/aop.html https://medium.com/engineering-zemoso/when-not-to-autowire-in-spring-spring-boot-93e6a01cb793" }, { "title": "New Workspace 2021", "url": "/posts/New-Workspace-2021/", "categories": "Projects", "tags": "personal, development", "date": "2021-08-21 02:30:00 +0800", "snippet": "Optum United Health Group GIFfrom Optum GIFsOrganization ChangeSo with the end of the Project at Infineon Technologies, Bangalore which was a Research Project for the company, and my third and four...", "content": "Optum United Health Group GIFfrom Optum GIFsOrganization ChangeSo with the end of the Project at Infineon Technologies, Bangalore which was a Research Project for the company, and my third and fourth semester of the MTech program, I would be moving from Semiconductor Industry to Healthcare Industry with Optum, UHG and I am greatly excited to transition into something impactful and backend engineering.‌Organization embracing the ML waveWith my little experience in the industry so far and what I heard from peers the organizations that are not 100% software product companies trying out various combinations of including machine learning in the existing line of products or use cases, so the organizations start with various projects spread across multiple teams but the only handful of them make it to a presentation room where their business value comes into existence rest are sometimes just sent to archives as a case study for what didn’t work or couldn’t work right now but will be viable few years ahead due to several reasons, this is valid enough reason not every problem or machine learning solution works as needed. The Other side of the coin is the hired team or Individual machine learning engineer that has to find a place or move into other projects as the ML project is being scraped, and into different technologies, and this may vary from organization to organization but it is a tend saw many of graduate interns being hired for different projects but months later end up doing the different project or department then what they have been learning up to.‌Pick Projects NOT CTCI don’t have much experience but both companies should hire people based on the project and individual need for the project and keep the preference and skillset in mind while assigning new projects and freshers out of college should pick the projects and work they like doing and see a future, rather than chasing the CTC numbers or brand name and then not liking their daily job.‌New Tech StackSince the start of the master’s program I have been spiraling down the machine learning and deep learning track from, college projects to word autocomplete using machine learning at the hackathon to my Face recognition match library for java cards along with more than a dozen ML courses from Coursera, but with my new work won’t be doing ML instead will be into pure backend engineering which is as cool as ML for me but the question arises what to do with the acquired ML knowledge, keep it on hold with regular revisions and hands-on for side projects and learning or just let it sit until I find some use case to fit ML into, well anything learned is never goes to waste and just broaden the scope of understanding, so one more feather of ML in the hat of jack of all master of none yet!‌Side Project Update (Solvepao.com)‌With complete new full-stack frameworks and a lot more things and skillset, I am learning at the workplace will be using the same for the side project instead of writing it in (C#, Blazor using web assembly ) as I started on it earlier so more delay on that but will at least put up a nice landing page for the domain, I purchased which has been just sitting there with ever-increasing countdown timer, so will use the project as a test bench to implement the new learning.‌What’s Next?Have a few side quests and learning projects and challenges, stacked up for me but will put them on hold as with my ongoing training in the company have already tons of new things to learn.‌Let’s see what’s next, will try to write more for sure Happy Learning :)" }, { "title": "Fixing my Hot Laptop", "url": "/posts/laptop-repair/", "categories": "Projects", "tags": "project, repair", "date": "2021-05-14 02:30:00 +0800", "snippet": "Fixing Overheating LaptopMy personal laptop is a Lenovo Thinkpad E430, 2012 Model which apart from its tough built has survived the time but many upgrade and repair to survive through college and e...", "content": "Fixing Overheating LaptopMy personal laptop is a Lenovo Thinkpad E430, 2012 Model which apart from its tough built has survived the time but many upgrade and repair to survive through college and even part school also, with $$ spent on both hardware and even appearance upgrade RAM Upgrade HDD replacement under Warranty HDD to SSD Upgrade New Skin and Stickers Battery Replacement New Charging AdapterEven with aging hardware specs it still gets the job done until I didn’t start its meltdown countdown by heavy tasks such as Video editing or Android Studio or games, so with most of the things in browser from Machine Learning to coding it was doing well, I did dual boot into Linux and underclocked the CPU Frequency in Summer to keep the temperature in safe Range or use cooling Pad in keeping the temperature down I even closed the laptop and kept it upside down with the external mouse, keyboard, and Monitor attached to let the room Air Cool its bottom and CPU temp monitor in every OS I booted in to keep a check on temperature which got me through past 2 yrs.Nearing MeltdownI had planned to buy a new system in 2020, which got delayed first due to start of covid and shortage of laptop specially thinkpads, later with Work laptop in place and using a cloud VM credit on Azure with college email ID got me further, and Apple’s release of Apple Silicon just made me hold longer as seeing my use case ARM-powered mac checked all boxes, fast forward to Summer 2021 the laptop is easily reaching 100 degree with even cooling pad and every trick with even medium workloads and even once failed to boot when overheated, running the laptop at that temp would sooner or later kill the CPU at this rateLet it Burndown ?I did clean it every few months for dust but didn’t give much thought to changing thermal pate as putting more $$ in the computer I was going to soon replace didn’t justify, until this week with the start of summer the laptop easily jumped to 100 degrees with even minimal workloads that needed to fix otherwise it was going to die for sure this summerSo with over the year accumulated knowledge of watchings 100’s of hours of Linus Tech Tips on Youtube, I found a thermal paste was easy knowing the popular brands and application techniques knowing thermal cooling pad for CPU aren’t an option and I needed thermal paste with zero electrical conductivity so that I don’t end up frying the components with spillage, without wanting to spend much $$ , Cooler MasterGel Regular seemed a good deal with me getting easy applicator packaging, trusted brand knowing it won’t wear out or cause any complication over time and especially a wipe with Alcohol included in package made it easier to clean the older paste knowing it won’t damage electrical componentsRepairthe repair went easier than thought and components were easy to take apart and for fun, I recorded the whole procedure Video on youtubeResultThe Idle temp is now about 45 Degree and even under max load the temp never reaches beyond 70 Degree even after hours of usage and no meltdown nowLet’s see how long it works without another technical issue until it is replaced by an ARM device :)" }, { "title": "100 Days - 100 PROJECTS CHALLENGE", "url": "/posts/100-days-100-projects/", "categories": "Projects", "tags": "project, coursera, development", "date": "2021-04-30 02:30:00 +0800", "snippet": "Day 0Last year when I did #100daysofMLcode #100daysofcode 🤔 (100 days of ML code is a subset of 100 days of code right 😅🤣) so I saw on various other #100 days even #365 other days challenge of bei...", "content": "Day 0Last year when I did #100daysofMLcode #100daysofcode 🤔 (100 days of ML code is a subset of 100 days of code right 😅🤣) so I saw on various other #100 days even #365 other days challenge of being consistent working on something one likesOne interesting challenge that caught my eye was #100daysofproject by @lindsayjeanthomsonPurposeThe #tag keyword was last 0.1% bit of push needed to take it as a challenge to widen my knowledge base every day about various tools and technology stack by hands-on learning, to help me later in rapid prototyping of ideas and side projects (maybe better system designs too ? ), otherwise once in office work these things take a back seat and broaden the knowledge base of tools and tricks helps in a lot of places over time and lastly, 1-2 hrs of the day before sleep are spent scrolling Twitter or Youtube, with multiple tabs that just get saved for next day, it’s said to pick a habit of reading before bed every day why not the habit of hands-on and learning something every dayRules I followed create a project every day without a miss a project could be: an app, a component, a website, a game, a library, hands-on tutorial following a youtube video or some guided tutorial anything, etc the task has to be done before the end of day every day for next 100 DaysHow to come with so many ideas or Projects?It’s simple: I am a genius 😎. ?Just kidding ?… I’m far from being that.The truth is over the years I have gathered a lot of interesting projects, posts, articles tutorials that I want to work on but never get to start them. and gathering and looking back to these projects and idea collection gives me an even more creative ideaAlso, doing this creative process over and over and over, you end up eventually getting ideas from all the things around you. There are some projects that I want to do because I see a need in my day-to-day life (those app ideas you get while brushing your teeth and before sleep) or the major mental roadblock you get when an idea comes up in head how to get it done/start with or to make that?For beginning with, thanks to my University providing students with coursera subscription, so all of the projects will be guided projects which will be helpful in learning and act as boilerplate templates and get in the flow.How much work did it take to create a project?Some of the projects I did in under 30 minutes, a couple took me 2-3 hours and 1 or 2 I had to “postpone” to the 2nd day because I couldn’t finish them. Although I postponed these projects to the next day, I recreated them from scratch - only using the gained knowledge. This turned out to be a good way to get “unstuck”. Just redo it from scratch.I would say that I needed around 1-1.5 hours on average per day. This includes: watching the hands-on guided material, setup &amp; installation, and googling stuff, and coding it out.Keep in mind that I’ve been coding since 2014, so topics I chose or the tech stack I opted for can vary from a wide variety of topics, but it depends on what interests you or what project you have in mind.What motivated me to keep going?Even after a Masters Degree and more than 1 year worth of industry experience in R&amp;D projects so many variables and unknown in my head existed in my mind map whenever I think of an app idea or project or some software product that curiosity of how does that work or how is that done keeps me going.What did I learned during the challenge?Apart from all the tech tools and tricks, there was a period in which there was information overload with too much diverse learning every day in too many domains, and all information was just getting harder to stitch together so I learned how to Learn Learning How to Learn: Powerful mental tools to help you master tough subjects which was good and I every college should keep it as optional or recommend or teach students how to learn before teaching. The course gave me amazing insights into how the mind works and how to improve and use it better, the course is free I recommend anyone who has a tiny bit of interest in how mind and thoughts work should check it out, that course helped me form a better mental model or scaffolding for future learning.What would you do differently?If I had to do this whole thing differently will make a project that is public and would want to probably screen record in the tutorial or share code or blog post for individual projects others can follow throughShould people do this challenge?If you have been wanting to learn something or form a habit and it been on the waitlist forever start with a friend or pick it as a challenge to get it doneProject List of 100 Daysbelow is the project list with the name of the hands-on guided project I completed from Coursera and its completion URL next to it with the date of completion on certificate (all 100 in continuation without a break, on 100th day, ended with having Covid) Day Project Name Certificate URL Day 1 Build a mobile app with Google Sheets on Glide and no coding Link Day 2 Python World Map Geovisualization Dashboard using Covid Data Link Day 3 Getting Started with Spatial Analysis in GeoDa Link Day 4 Start Your API Testing Journey With Postman Tool Link Day 5 Geo-Visualization in Python Link Day 6 Automation Scripts Using Bash Link Day 7 Climate Geospatial Analysis on Python with Xarray Link Day 8 COVID19 Data Visualization Using Python Link Day 9 Python Geospatial Data Analysis Link Day 10 Introduction to Spatial SQL with PostGIS Link Day 11 Python Tricks and Hacks for Productivity Link Day 12 Design and Develop a Website using Figma and CSS Link Day 13 Build a Data Science Web App with Streamlit and Python Link Day 14 Geospatial Big Data Visualization with Kepler GL Link Day 15 Create Interactive Dashboards with Streamlit and Python Link Day 16 Data Visualization with Plotly Express Link Day 17 Schedule Cron Job on Google App Engine Link Day 18 Create RESTful APIs for Spotify using Postman Link Day 19 Build a Google Firebase Web Application Link Day 20 Getting Started With Google Apps Script Link Day 21 Testing and Debugging Python Link Day 22 Emotion AI: Facial Key-points Detection Link Day 23 Image Processing with Python Link Day 24 Generate API Documentation from Postman Link Day 25 Data Visualization with Python Link Day 26 Interactive Machine Learning Dashboards using Plotly Dash Link Day 27 Creating a Wordcloud using NLP and TF-IDF in Python Link Day 29 Merge, Sort and Filter Data in Python Pandas Link Day 30 Processing Data with Python Link Day 31 Introduction to Docker : The Basics Link Day 32 Docker Essentials &amp; Building a Containerized Web Application Link Day 33 Introduction to Docker: Build Your Own Portfolio Site Link Day 34 Create Docker Container with Flask Seaborn Regression Plot App Link Day 35 Create Your First Chatbot with Rasa and Python Link Day 36 Introduction to Amazon Web Services (AWS) Link Day 37 Image Super Resolution Using Autoencoders in Keras Link Day 38 Machine Learning with Docker Link Day 39 Deploy Models with TensorFlow Serving and Flask Link Day 40 Containerization Using Docker Link Day 41 TensorFlow Serving with Docker for Model Deployment Link Day 42 Siamese Network with Triplet Loss in Keras Link Day 43 Automatic Machine Learning with H2O AutoML and Python Link Day 44 Machine Learning with H2O Flow Link Day 45 Mining Data to Extract and Visualize Insights in Python Link Day 46 Predict Future Product Prices Using Facebook Prophet Link Day 47 Clustering Geolocation Data Intelligently in Python Link Day 48 Build CRUD REST API in Django Link Day 49 Deploy a BERT question answering bot on Django Link Day 50 Analyzing Video with OpenCV and NumPy Link Day 51 Jenkins : Automating your delivery pipeline Link Day 52 Database Creation and Modeling using MYSQL Workbench Link Day 53 Computer Vision - Object Detection with OpenCV and Python Link Day 54 Video Basics with OpenCV and Python Link Day 55 Facial Expression Recognition with Keras Link Day 56 Perform Real-Time Object Detection with YOLOv3 Link Day 57 Python OpenCV Motion Detection Link Day 58 Build local development environments using Docker containers Link Day 59 Classification Trees in Python, From Start To Finish Link Day 60 Machine Learning Feature Selection in Python Link Day 61 Network Data Science with NetworkX and Python Link Day 62 Use Python to Create a Web Testing Bot Link Day 63 Determine Shortest Paths Between Routers Using Python Link Day 64 Simple Nearest Neighbors Regression and Classification Link Day 65 Introduction to Customer Segmentation in Python Link Day 66 Use Jenkins to Automate Software Build and Test Link Day 67 Build a film club web app on Google AppEngine Link Day 68 Database Design with SQL Server Management Studio (SSMS) Link Day 69 Create Your First Web App with Python and Flask Link Day 70 Sentimental Analysis on COVID-19 Tweets using python Link Day 71 Create a Google Chrome extension Link Day 71 Compose and Program Music in Python using Earsketch Link Day 72 Interactive Geospatial Visualization:Kepler GL &amp; Jupyter Lab Link Day 73 Personal Desktop Notifier in Python: Covid-19 notifications Link Day 74 Data Analysis Using Pyspark Link Day 75 Medical Diagnosis using Support Vector Machines Link Day 76 Build a Bot in Python for Basic File and Interface Chores Link Day 77 Write your own Python tool to footprint a web application Link Day 78 Build a Working Chatbot in Python Link Day 79 Building Machine Learning Pipelines in PySpark MLlib Link Day 80 Create a Dynamic-Link Library with DevC++ for Python Link Day 81 Linux: Archiving and Compression for DevOps (tar/gzip) Link Day 82 Building Test Automation Framework - Selenium, C# &amp; NUnit Link Day 83 Human Predicament Complex Modeling Link Day 84 Building Similarity Based Recommendation System Link Day 85 Simulating Time Series Data by Parallel Computing in Python Link Day 86 Build a Recommender System in Python Link Day 87 Getting Started with Power BI Desktop Link Day 88 User Interface (UI) Design with Wireframes in Miro Link Day 89 Plan Projects and Brainstorm with Mind Maps in Miro Link Day 90 The Product Lifecycle: A Guide from start to finish Link Day 91 Introduction to Node-red Link Day 92 Deep Learning with PyTorch: Build a Neural Network Link Day 93 TensorFlow for AI: Get to Know Tensorflow Link Day 94 Monitoring &amp; Telemetry for Production Systems Link Day 95 Storytelling With Data Link Day 96 Satellite Imagery Analysis in Python Link Day 97 AWS S3 Basics Link Day 98 AWS Lambda and API Gateway Basics - Build Serverless website Link Day 99 AWS Elastic Beanstalk:Deploy a Python(Flask) Web Application Link Day 100 Object Detection with Amazon Sagemaker Link PS (continue learning but less frequently): 101 Create Your First NoSQL Database with MongoDB and Compass Link What’s next for Me?Probably launch the public beta of my side project before the end of this year 🤞.DSA , ML ? , .NET Core ? lets see" }, { "title": "Indian Dataset for Data Science Projects", "url": "/posts/indian-data-source/", "categories": "Projects", "tags": "project, Machine Learning, Indian Dataset", "date": "2021-01-29 02:30:00 +0800", "snippet": "Data SourcesSlide from Ai- For Everyone by Deeplearning.ai on Coursera by Andrew NgStarting on my Plan to Build real-world project that serves a function solvepao.com the main initial data I needed...", "content": "Data SourcesSlide from Ai- For Everyone by Deeplearning.ai on Coursera by Andrew NgStarting on my Plan to Build real-world project that serves a function solvepao.com the main initial data I needed was GIS data of various coordinates of public utility like school, education, Hospitals, etc for India to start with. These Data sources for other countries are easily available online and used as learning dataset for beginner data science projects and hosted on all major public data set repository, apparently for India there is a rise in the initiative by the government to make the data publicly available, and the NIC (National Informatics Center) working on various projects to make that data more accessible but the GIS data I was looking for is only available in the public domain to preview using Bharat Map but not for download, while I was Googling for Data but I did compile a list in the processFew really good examples to check out are : https://stategisportal.nic.in/stategisportal/ https://bhuvan.nrsc.gov.in/bhuvan_links.php https://indiabiodiversity.org/?lang=enPS: If you look for other specific data like IRIS data for Indian citizens, street signs, handwriting, and other biomedical data that can be found on various University project sites in Indiahere is compiled a list of various data sources one can check out when working on the next data science project :1. Data.Gov.InIt publishes datasets, documents, tools, and applications collected by the government for public use and community participation of the products with visualization, APIs, alerts, etc. It is also a collection of all the government based datasets discussed Below.Link: https://data.gov.in/2. National Portal of IndiaIt was developed by the Indian government to facilitate single window access to information and services of all government entities, A single-point access to a lot of information, has a searchable contact directory, a database of the government website, and othersLink: https://www.india.gov.in/3. Ministry Of Statistics And Programme Implementation DatasetThe datasets are collected by conducting large-scale sample surveys across India for various parameters, which eventually leads to the creation of the database. The ministry applies standard statistical techniques and large inspection and supervision to enable this.Link: http://mospi.nic.in/data4. RBI Database Of Indian EconomyIt is loaded with suitable information and data for researchers, analysts, and general users all alike. It has datasets across money and banking, financial markets, national income, saving and employment, and others. The idea is to make easy present-day styles of data analysis that can provide important real-time numbers about economic activity, prices, and more.Link: https://dbie.rbi.org.in/DBIE/dbie.rbi?site=home5. Gateway To Indian Earth ObservationAn initiative by ISRO, the open data archive(records) provides free satellite data, products download facility, and thematic datasets. It uses a crowdsourcing approach to collect enriching and point-of-interest data. It also acts as a platform to host government data such as the forest department. Apart from being a repository of data, it allows users to explore the 2D and 3D representation of the surface of the earth, pest surveillance, disaster services, high-resolution imagery of cities, among others.Link: https://bhuvan.nrsc.gov.in/bhuvan_links.php#6. India Weather DataWith datasets for various meteoroid indicators(measures), water resource planning, rainfall, and others from across various parts of India, these datasets are available for users in simple formats. It also contains databases for several other parameters such as temperature, pressure, relative humidity, precipitation amount, wind speed, solar radiation, among others.Link (Freemium) : https://www.meteoblue.com/en/weather/archive/export/india_el-salvador_35854817. Aadhaar MetadataThis provides a huge database generated by the daily count of total registrations, enrolment applications accepted and rejected by state and district. It also contains other details such as Aadhaar generated by age, gender, etcLink: https://data.gov.in/dataset-group-name/aadhaar8. Import Exports DatasetsThe Indian Customs Electronic Commerce(IceGate)/Electronic Data Interchange Gateway is a portal with e-filling services for trade and cargo carriers. It also has an in-depth National Import Database (NIDB) and Export Commodity Database (ECDB) for the Directorate of valuation that is being handled by the IceGate. It has information such as documents, messages, and other processes by the customs end by the Indian Customs EDI System (ICES)Link: https://www.icegate.gov.in/jsp/DailyReport.jsp9. Wildlife Institute of India DatasetAn autonomous institution under the Ministry of Environment Forest and Climate change, the Government of India, has datasets on different wildlife species in India. There are a total of 4591 specimens that are housed at WII herbarium, of which 4322 are digitized and published through the GBIF network. The data is mainly used by researchers and field managers from the respective protected areas of the country to prepare for the management plan and other research.Link : WII Herbarium Dataset, GBIF.10. Open Data TelanganaVarious Data sets on multiple domains, Industry, Sectors, and Agricultural Data for the State of TelanganaLink: https://data.telangana.gov.in/search/type/dataset11. National Data RepositoryA Data repository on Seismic Data, Well &amp; Log Data, Spatial Data, Other G&amp;G data like Drilling, Reservoir, Production, Geological, Gravity &amp; Magnetic, etc. Reports and DocumentsLink: https://www.ndrdgh.gov.in/NDR/12. IndiastatA Private owned site that combines and categorize data from the above data sourcesLink: https://www.indiastat.com/13. India Biodiversity PortalFree and open access to India’s biodiversity informationA unique repository of information on India’s biodiversity. The Portal aims to aggregate data through public participation and provide open and free access to biodiversity informationLink: https://indiabiodiversity.org/?lang=en14. National Health Systems Resource CenterA Technical Support Institute with National Health Mission with datasets on HealthcareLink: http://nhsrcindia.org/health-systems-database15. Central Library Indian Statistical InstituteLibraryIn line with the objectives of the institute, over the years, the library has developed a comprehensive collection of peer-reviewed scholarly literature useful for the faculty and the research community of the institute. The other objective is to serve as a resource center for the scholars and scientific community of the country.Link: https://www.isical.ac.in/~library/data.php16. Bharat MapNIC/DeitY has created a Multi-Layer GIS Platform named “Bharat Maps” which depicts core foundation data as “NICMAPS”, an integrated base map service using 1:50,000 scale reference data from Survey of India, ISRO, FSI, RGI, and so on. This encompasses 23 layers containing administrative boundaries, transport layers such as roads &amp; railways, forest layers, settlement locations, etc., including terrain map services.Link: https://stategisportal.nic.in/stategisportal/Link: https://bharatmaps.gov.in/Link: https://bhuvan.nrsc.gov.in/bhuvan_links.php💡 Find or Gather your own data and try to make some use of out it , the main learning lies in the process" }, { "title": "Machine Learning Samosa", "url": "/posts/samosa-ml/", "categories": "Projects", "tags": "project, Machine Learning, NLP", "date": "2020-12-11 02:30:00 +0800", "snippet": "Project S.A.M.O.S.ASmart Autocomplete Machine Optimised Search Assistant 😋So one fine evening of 23 September 2020 an Office Mail arrived announcing the Infineon iHack 2020 in which there were mult...", "content": "Project S.A.M.O.S.ASmart Autocomplete Machine Optimised Search Assistant 😋So one fine evening of 23 September 2020 an Office Mail arrived announcing the Infineon iHack 2020 in which there were multiple tracks for each domain and had a specific problem statement for each track one had to build a solution for in a team.Armed with my ML knowledge since the start of 2020 I quickly formed a team with my friends Shraddha, Dhanushri, Shreeshaand since we had some idea about image processing and ML and statistical data but no clue on applied NLP ML, we started with what was most important ………deciding a cool name for project, it was rainy season in Delhi and in sept and mostly our project discussion were on weekends hence it magically landed on the word samosa, the full form of it came afterwardSo here is how we approached the problem of building an autocomplete ML model from scratch and won as 2nd runner up in the first-ever Machine Learning Hackathon.Step 1 - Understanding the Problem StatementSo our problem statement was to create an ML model trained on a given dataset that will generate the most accurate and relevant autocomplete give a uni-gram or bi-gram eg if the dataset includes an article on semiconductor then typing “Semiconductor” and the partial world “in” should give out [industry,intel, insulator, inductor] to achieve what we use in google autocomplete suggestion that came in 2010 aroundStep 2 - Knowledge Ramp-upSince we had 2 months of time to prepare and no practical NLP model hands-on to get an idea of what we were getting into , we started looking up various approaches that can be used to solve the problem , we all had prior knowledge of Tf-IDF and Markov chain which were part of NLP course in University but lot of buzzwords GPT-3 and other advanced model made us doubt ourself that there might be a better solution to it that we don’t know and since the other objective apart from solving the statement was learning too so we all set out to learn from whatever resource we could from medium articles , Youtube , Coursera in which i ended up doing entire 4 course specialization for remaining 80% if time until event to know the complete start to end of Natural language processing from state of the Art models to simple sentiment analysis and word count techniques, Here is my certificate from Coursera for it which was the collection of following courses : Natural Language Processing with Classification and Vector Spaces Natural Language Processing with Probabilistic Models Natural Language Processing with Sequence Models Natural Language Processing with Attention ModelsAfter taking a complete tour of the landscape of NLP for finding the best (kickass) solution to fit our problem we decided to use a Hybrid approach to use Tf-Idf and Word2vec for our solution. As a regular podcast listener, I also gained some really useful and relevant suggestions and brain food for the project from the Podcast Practical Ai by Chris Benson &amp; Daniel Whitenack for how NLP problems are solved at scale in the industryPS : For learning, I also completed Google 30 Days of Google cloud program to learn how to deploy and scale ML models on the cloud, for which i got goodies and Certificate , and Learning badegeStep 3 - Building the ModelI could write on a nice technical article on how I built my model (in short it was word2vec + Tf-Idf combined, if you want details to leave a comment below I’ll send my notes and whiteboard diagrams) but the real good takeaways and learning/challenges that I solved were How to efficiently save the models and load them back in memory and various file formats and their performance Performance difference between pandas and Numpy How much Cleaning and knowing your data is important and 80% of work in NLP No matter what another fancy state of the art solution you may find online, (as boring as it may sound and to explain to your colleagues or write on your resume )some custom solution based on classical known technique can give you a better and efficient solution if you know your data well. It’s every programmer’s dream to have a supercomputer equivalent processing power in their machine but try to use the cloud instead of burning your system down, especially for learning and side projects. Just like we have Github for code versioning there is Data version Control and other tools to be explored Make notes of what works to what not works and when reading some paper note down terms you don’t understand and look it up laterLastly, write down the approach and your working as you build if you know you are going to write about it and don’t have to sum up in short like i did 🙈 and can write a good technical article about it.PS: Added some articles I found online notes in reference at the end.Step 3.5 - Teamwork SkillsIn every project there is and should be a team lead or project in-charge and not pretend like no one is the team leader and everyone is in charge, being the one in this team and for other projects over years as a person in charge here are few things one should always take care off. Listen to every idea before you present your own Know the strong and weak areas of every team member and divide work accordingly. Meeting can be a real-time blackhole so set an agenda before the meeting or tell others in advance the topics to be discussed in teams and reschedule if nothing Important is to be discussed or even 1 member is missing. Make sure everyone gets a chance to what they want to work on and everyone does work and don’t keep members who just do MBA 101 stuff more than actual relevant work to an engineering project.Step 4 - PresentGive a nice crisp presentation and don’t worry about the prize it’s learning that matters in long run.Eat the Samosa (actual ones when you win)🥳Reference https://towardsdatascience.com/how-we-built-an-ai-powered-search-engine-without-being-google-5ad93e5a8591 https://juan0001.github.io/next-word-prediction/ https://towardsdatascience.com/next-word-prediction-with-nlp-and-deep-learning-48b9fe0a17bf https://towardsdatascience.com/exploring-the-next-word-predictor-5e22aeb85d8f https://towardsdatascience.com/next-word-prediction-with-nlp-and-deep-learning-48b9fe0a17bf#:~:text=The%20model%20will%20consider%20the,pre-processing%20of%20the%20data https://github.com/starlordvk/Typing-Assistant" }, { "title": "Hello World 2020", "url": "/posts/hello-world/", "categories": "Projects", "tags": "jekyll, personal, Github, Blogging", "date": "2020-12-10 16:40:00 +0800", "snippet": "New Blog SetupHello Reader , for long it has been pending on my list to create a personal portfolio website , to showcase my project , writeblogposts , twitter integration etc etc but it didnt fit ...", "content": "New Blog SetupHello Reader , for long it has been pending on my list to create a personal portfolio website , to showcase my project , writeblogposts , twitter integration etc etc but it didnt fit well as being a software developerand many projects being done for company there arent isnt much to show apart from some general lessons or findings as a developer , I did try posting daily progress when i pushed the pedal to the metal for learning the machine learning in which daily become weekly to monthly , and its better writing 1 high quality post than truckload of smaller ones which stays one the internet for long with google killing so many products and blogger going so many changes , it was time to move on to github pagesChoice of Framework for Github Static pagesEalier this year i gave a tried setting up jekyll (Ruby) from scratch but couldnt find the right theme to pick for , then i setup Mkdocs (python) for making my note collection and code snippets which worked well for a while but didnt fit my usecase and the Github Workflow broke after sometime i didnt get time to fix and starting using Gitbooks for notes and writing markdownsUntil December i cam across this wonderfull theme by Cotes which was the ….light bulb momenet of this was what i was looking for , should really checkout and Sponser this project and the setup and documentation is so well done i was up and running and no timeDiscontinuing Blogger and MediumMedium and Blogger may have better indexing and ranking in search results but Medium Signin/Paywall and free article count and google everchanging product futures seemed not so promisingI do recommend to use Dev.to , Hashnode for setup tutorials and general articlesTakeaway Go opensource Quality over Quantity Paywalls and Signin [meh]" }, { "title": 2020, "url": "/posts/2020/", "categories": "Blogger", "tags": "personal, Bloger, Blogging", "date": "2020-01-01 02:30:00 +0800", "snippet": "All posts I wrote in 2020 in Bloggerwritten on harshityadav95.blogspot.com Endgame 2020 Course Checkpoint : Ai for Everybody by Andrew Ng on Coursera #100 Days of Code - Initialize….. 100 Days ...", "content": "All posts I wrote in 2020 in Bloggerwritten on harshityadav95.blogspot.com Endgame 2020 Course Checkpoint : Ai for Everybody by Andrew Ng on Coursera #100 Days of Code - Initialize….. 100 Days of Machine Learning Code Day 1 100 Days of Machine Learning Code Day 2 100 Days of Machine Learning Code Day 3 100 Days of Machine Learning Code Day 4 100 Days of Machine Learning Code Day 5 100 Days of Machine Learning Code Day 6 100 Days of Machine Learning Code Day 7 100 Days of Machine Learning Code Day 8 100 Days of Machine Learning Code Day 9 100 Days of Machine Learning Code Day 10 100 Days of Machine Learning Code Day 11 100 Days of Machine Learning Code Day 12 100 Days of Machine Learning Code Day 13 100 Days of Machine Learning Code Day 14 100 Days of Machine Learning Code Day 15 Break of 39 Days , Digitalization Driven by Covid-19 in Education Sector Days of Machine Learning Code Day 16-28 Days of Machine Learning Code Day 28-61 https://harshityadav95.blogspot.com/2020/09/day-66-100-playing-with-face-recognition.html" }, { "title": 2019, "url": "/posts/2019/", "categories": "Blogger", "tags": "personal, Bloger, Blogging", "date": "2019-01-01 02:30:00 +0800", "snippet": "All posts I wrote in 2019 in Bloggerwritten on harshityadav95.blogspot.com Brush up With GitHub Learning Lab Repurpose/Reuse of Old Android Tablet of 2013 in 2019 Changing to Speed Lane : Relian...", "content": "All posts I wrote in 2019 in Bloggerwritten on harshityadav95.blogspot.com Brush up With GitHub Learning Lab Repurpose/Reuse of Old Android Tablet of 2013 in 2019 Changing to Speed Lane : Reliance Jio GigaFiber / JioFiber Installation and Setup Secure and Private AI Scholarship Challenge Nanodegree Program" }, { "title": 2018, "url": "/posts/2018/", "categories": "Blogger", "tags": "personal, Bloger, Blogging", "date": "2018-01-01 02:30:00 +0800", "snippet": "All posts I wrote in 2018 in Bloggerwritten on harshityadav95.blogspot.com", "content": "All posts I wrote in 2018 in Bloggerwritten on harshityadav95.blogspot.com" }, { "title": 2017, "url": "/posts/2017/", "categories": "Blogger", "tags": "personal, Bloger, Blogging", "date": "2017-01-01 02:30:00 +0800", "snippet": "All posts I wrote in 2017 in Bloggerwritten on harshityadav95.blogspot.com Long Time Django - Marathon 6 Hours (Start) Django-Marathon 1/6 Django-Marathon 2/6 Django -Marathon 3/6 Django –Mar...", "content": "All posts I wrote in 2017 in Bloggerwritten on harshityadav95.blogspot.com Long Time Django - Marathon 6 Hours (Start) Django-Marathon 1/6 Django-Marathon 2/6 Django -Marathon 3/6 Django –Marathon 4/6 Django -Marathon 5/6 Django –Marathon 6/6 Overview MVA Django Web App Marathon Smart India Hackathon 2017 Network Speed Units Smart India Hackathon 2017 - Event MVA Know It Prove it - Web Development MVA Know it prove it- Web Development -1 Getting Started with Web Technologies MVA Know it prove it- Web Development -2 HTML5 &amp; CSS3 - i MVA Know it prove it- Web Development -2 HTML5 &amp; CSS3 - ii MVA Know it prove it- Web Development -2 HTML5 &amp; CSS3 - iii Basic Git Commands Difference between AI , Machine Learning and Deep Learning Notes :Programming in C# Jump Start ( Microsoft Exam-70-483) Top 20 C# Questions IMAD Hasura — 8 Weeks Course IMAD Hasura — Week 1 IMAD Hasura — Week 2 The Angular Day -3/9/17 My Last Workshop to Jr. in College" }, { "title": 2016, "url": "/posts/2016/", "categories": "Blogger", "tags": "personal, Bloger, Blogging", "date": "2016-01-01 02:30:00 +0800", "snippet": "All posts I wrote in 2016 in Bloggerwritten on harshityadav95.blogspot.com My Blog- Beginning Backups ,System Updates &amp; Partitioning Time to Gear Up Day 1: Starting Python Using AWS For Fi...", "content": "All posts I wrote in 2016 in Bloggerwritten on harshityadav95.blogspot.com My Blog- Beginning Backups ,System Updates &amp; Partitioning Time to Gear Up Day 1: Starting Python Using AWS For First Time Disk Crash ! Hard Drive Failure ! Skil-set Progress : Python" } ]
